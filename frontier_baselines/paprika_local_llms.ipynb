{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be6e8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d296088",
   "metadata": {},
   "source": [
    "So let us do 50 games for each with frontier LLMs - one with reasoning and one without.\n",
    "\n",
    "# 2. Setup LLM Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16882ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-12 21:09:53] server_args=ServerArgs(model_path='Qwen/Qwen2.5-7B-instruct', tokenizer_path='Qwen/Qwen2.5-7B-instruct', tokenizer_mode='auto', skip_tokenizer_init=False, load_format='auto', trust_remote_code=False, dtype='auto', kv_cache_dtype='auto', quantization=None, quantization_param_path=None, context_length=None, device='cuda', served_model_name='Qwen/Qwen2.5-7B-instruct', chat_template=None, completion_template=None, is_embedding=False, enable_multimodal=None, revision=None, host='0.0.0.0', port=35905, mem_fraction_static=0.88, max_running_requests=None, max_total_tokens=None, chunked_prefill_size=8192, max_prefill_tokens=16384, schedule_policy='fcfs', schedule_conservativeness=1.0, cpu_offload_gb=0, page_size=1, tp_size=1, pp_size=1, max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=883530097, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, log_level='info', log_level_http=None, log_requests=False, log_requests_level=0, show_time_cost=False, enable_metrics=False, bucket_time_to_first_token=None, bucket_e2e_request_latency=None, bucket_inter_token_latency=None, collect_tokens_histogram=False, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, api_key=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, dp_size=1, load_balance_method='round_robin', ep_size=1, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, lora_paths=None, max_loras_per_batch=8, lora_backend='triton', attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', speculative_algorithm=None, speculative_draft_model_path=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, disable_radix_cache=False, disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_nccl_nvls=False, enable_tokenizer_batch_encode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_ep_moe=False, enable_deepep_moe=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, eplb_rebalance_num_iterations=1000, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=None, enable_expert_distribution_metrics=False, deepep_config=None, enable_torch_compile=False, torch_compile_max_bs=32, cuda_graph_max_bs=None, cuda_graph_bs=None, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, allow_auto_truncate=False, enable_custom_logit_processor=False, tool_call_parser=None, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through_selective', flashinfer_mla_disable_ragged=False, warmups=None, moe_dense_tp_size=None, n_share_experts_fusion=0, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, mm_attention_backend=None, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_bootstrap_port=8998, disaggregation_transfer_backend='mooncake', disaggregation_ib_device=None, pdlb_url=None)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "from sglang.test.test_utils import is_in_ci\n",
    "from sglang.utils import wait_for_server, print_highlight, terminate_process\n",
    "\n",
    "if is_in_ci():\n",
    "    from patch import launch_server_cmd\n",
    "else:\n",
    "    from sglang.utils import launch_server_cmd\n",
    "\n",
    "# This is equivalent to running the following command in your terminal\n",
    "\n",
    "# python3 -m sglang.launch_server --model-path qwen/qwen2.5-0.5b --host 0.0.0.0\n",
    "local_model_name = \"Qwen/Qwen2.5-7B-instruct\" # \"qwen/qwen3-4b\" # \"meta-llama/Llama-3.3-70B-Instruct\" # \"unsloth/Llama-3.3-70B-Instruct-bnb-4bit\"-- assert issue #  \"Qwen/Qwen2.5-3B-Instruct\" # \"Qwen/QwQ-32B\"\n",
    "#\n",
    "tp_size = (len(os.environ[\"CUDA_VISIBLE_DEVICES\"]) + 1) // 2\n",
    "server_process, port = launch_server_cmd(\n",
    "    f\"\"\"\n",
    "python3 -m sglang.launch_server --model-path {local_model_name} --host 0.0.0.0 --tp {tp_size}\n",
    "\"\"\"# --tp {tp_size} mem_fraction_static=0.4 \n",
    ")\n",
    "wait_for_server(f\"http://localhost:{port}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "636e9f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../../src/optimal_explorer')\n",
    "from llm_utils import llm_call\n",
    "from pprint import pprint as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c11f7137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-12 20:38:10] Prefill batch. #new-seq: 1, #new-token: 53, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0\n",
      "[2025-08-12 20:38:11] Decode batch. #running-req: 1, #token: 86, token usage: 0.00, cuda graph: True, gen throughput (token/s): 1.90, #queue-req: 0\n",
      "[2025-08-12 20:38:13] Decode batch. #running-req: 1, #token: 126, token usage: 0.00, cuda graph: True, gen throughput (token/s): 23.70, #queue-req: 0\n",
      "[2025-08-12 20:38:13] INFO:     127.0.0.1:50288 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "out = await llm_call(\n",
    "    model=local_model_name,\n",
    "    temperature=1,\n",
    "    url = f\"http://localhost:{port}/v1/chat/completions\",\n",
    "    get_everything=True,\n",
    "    reasoning_effort='high',\n",
    "    messages = [\n",
    "            {\"role\": \"system\", \"content\": 'You are a useless assistant that gives humorous answers.'},\n",
    "            {\"role\": \"user\", \"content\": 'Why is the sky blue?'},\n",
    "            {\"role\": \"assistant\", \"content\": 'It is because of the great king of Pokemons.'},\n",
    "            {\"role\": \"user\", \"content\": 'How so?'},\n",
    "        ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffa22a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set PORT in run_baselines_paprika_frontier.py to the port number here:\n",
    "print(port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767e1533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start running.\n",
      "[2025-08-12 20:40:57] Prefill batch. #new-seq: 1, #new-token: 223, #cached-token: 6, token usage: 0.00, #running-req: 0, #queue-req: 0\n",
      "[2025-08-12 20:40:57] Prefill batch. #new-seq: 2, #new-token: 439, #cached-token: 12, token usage: 0.00, #running-req: 1, #queue-req: 0\n",
      "2025-08-12 20:40:57,781 - INFO - flashinfer.jit: Loading JIT ops: cascade\n",
      "2025-08-12 20:40:57,892 - INFO - flashinfer.jit: Finished loading JIT ops: cascade\n",
      "[2025-08-12 20:40:58] Prefill batch. #new-seq: 15, #new-token: 1065, #cached-token: 2335, token usage: 0.01, #running-req: 3, #queue-req: 12\n",
      "[2025-08-12 20:40:59] Decode batch. #running-req: 18, #token: 981, token usage: 0.01, cuda graph: True, gen throughput (token/s): 3.21, #queue-req: 12\n",
      "[2025-08-12 20:41:01] INFO:     127.0.0.1:49432 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:01] INFO:     127.0.0.1:49458 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:01] INFO:     127.0.0.1:49446 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:01] Prefill batch. #new-seq: 7, #new-token: 7, #cached-token: 1569, token usage: 0.02, #running-req: 16, #queue-req: 5\n",
      "[2025-08-12 20:41:01] Prefill batch. #new-seq: 4, #new-token: 4, #cached-token: 905, token usage: 0.02, #running-req: 22, #queue-req: 1\n",
      "[2025-08-12 20:41:01] INFO:     127.0.0.1:49544 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:01] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 234, token usage: 0.02, #running-req: 25, #queue-req: 0\n",
      "[2025-08-12 20:41:01] Prefill batch. #new-seq: 1, #new-token: 197, #cached-token: 95, token usage: 0.02, #running-req: 26, #queue-req: 0\n",
      "[2025-08-12 20:41:01] INFO:     127.0.0.1:49520 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:01] Prefill batch. #new-seq: 1, #new-token: 120, #cached-token: 173, token usage: 0.02, #running-req: 26, #queue-req: 1\n",
      "[2025-08-12 20:41:01] Decode batch. #running-req: 27, #token: 1753, token usage: 0.02, cuda graph: True, gen throughput (token/s): 363.35, #queue-req: 3\n",
      "[2025-08-12 20:41:01] INFO:     127.0.0.1:49434 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:01] Prefill batch. #new-seq: 1, #new-token: 109, #cached-token: 185, token usage: 0.02, #running-req: 26, #queue-req: 2\n",
      "[2025-08-12 20:41:01] INFO:     127.0.0.1:49516 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:02] Prefill batch. #new-seq: 1, #new-token: 186, #cached-token: 102, token usage: 0.02, #running-req: 26, #queue-req: 1\n",
      "[2025-08-12 20:41:02] INFO:     127.0.0.1:49552 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:02] Prefill batch. #new-seq: 1, #new-token: 94, #cached-token: 198, token usage: 0.03, #running-req: 26, #queue-req: 2\n",
      "[2025-08-12 20:41:02] INFO:     127.0.0.1:49558 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:02] Prefill batch. #new-seq: 1, #new-token: 95, #cached-token: 193, token usage: 0.03, #running-req: 26, #queue-req: 2\n",
      "[2025-08-12 20:41:02] INFO:     127.0.0.1:49508 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:02] Prefill batch. #new-seq: 1, #new-token: 71, #cached-token: 221, token usage: 0.03, #running-req: 26, #queue-req: 1\n",
      "[2025-08-12 20:41:02] INFO:     127.0.0.1:49464 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:02] Prefill batch. #new-seq: 1, #new-token: 83, #cached-token: 205, token usage: 0.03, #running-req: 26, #queue-req: 2\n",
      "[2025-08-12 20:41:02] INFO:     127.0.0.1:49504 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:02] INFO:     127.0.0.1:49528 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:02] Prefill batch. #new-seq: 2, #new-token: 137, #cached-token: 435, token usage: 0.03, #running-req: 25, #queue-req: 0\n",
      "[2025-08-12 20:41:02] Prefill batch. #new-seq: 1, #new-token: 95, #cached-token: 194, token usage: 0.03, #running-req: 27, #queue-req: 2\n",
      "[2025-08-12 20:41:03] INFO:     127.0.0.1:49532 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:03] Prefill batch. #new-seq: 1, #new-token: 115, #cached-token: 181, token usage: 0.03, #running-req: 27, #queue-req: 1\n",
      "[2025-08-12 20:41:03] INFO:     127.0.0.1:49492 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:03] INFO:     127.0.0.1:49512 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:03] Prefill batch. #new-seq: 1, #new-token: 66, #cached-token: 223, token usage: 0.03, #running-req: 26, #queue-req: 1\n",
      "[2025-08-12 20:41:03] INFO:     127.0.0.1:49478 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:03] Prefill batch. #new-seq: 1, #new-token: 102, #cached-token: 190, token usage: 0.03, #running-req: 26, #queue-req: 0\n",
      "[2025-08-12 20:41:03] Prefill batch. #new-seq: 1, #new-token: 91, #cached-token: 201, token usage: 0.04, #running-req: 27, #queue-req: 2\n",
      "[2025-08-12 20:41:04] INFO:     127.0.0.1:49560 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:04] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 283, token usage: 0.04, #running-req: 27, #queue-req: 1\n",
      "[2025-08-12 20:41:04] Decode batch. #running-req: 28, #token: 3010, token usage: 0.04, cuda graph: True, gen throughput (token/s): 434.05, #queue-req: 2\n",
      "[2025-08-12 20:41:05] INFO:     127.0.0.1:49588 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:05] Prefill batch. #new-seq: 2, #new-token: 80, #cached-token: 502, token usage: 0.05, #running-req: 27, #queue-req: 0\n",
      "[2025-08-12 20:41:05] INFO:     127.0.0.1:49566 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:05] INFO:     127.0.0.1:49630 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:05] INFO:     127.0.0.1:49636 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:05] Prefill batch. #new-seq: 1, #new-token: 58, #cached-token: 234, token usage: 0.05, #running-req: 26, #queue-req: 0\n",
      "[2025-08-12 20:41:05] INFO:     127.0.0.1:49640 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:05] INFO:     127.0.0.1:49600 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:06] INFO:     127.0.0.1:49614 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:06] Prefill batch. #new-seq: 3, #new-token: 220, #cached-token: 658, token usage: 0.05, #running-req: 24, #queue-req: 0\n",
      "[2025-08-12 20:41:06] INFO:     127.0.0.1:49570 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:06] Prefill batch. #new-seq: 2, #new-token: 124, #cached-token: 461, token usage: 0.05, #running-req: 27, #queue-req: 0\n",
      "[2025-08-12 20:41:06] Prefill batch. #new-seq: 1, #new-token: 67, #cached-token: 217, token usage: 0.05, #running-req: 28, #queue-req: 1\n",
      "[2025-08-12 20:41:06] Decode batch. #running-req: 29, #token: 3947, token usage: 0.05, cuda graph: True, gen throughput (token/s): 517.36, #queue-req: 1\n",
      "[2025-08-12 20:41:06] INFO:     127.0.0.1:49590 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:06] Prefill batch. #new-seq: 1, #new-token: 75, #cached-token: 221, token usage: 0.05, #running-req: 28, #queue-req: 0\n",
      "[2025-08-12 20:41:06] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 283, token usage: 0.05, #running-req: 29, #queue-req: 0\n",
      "[2025-08-12 20:41:06] INFO:     127.0.0.1:49666 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:07] Prefill batch. #new-seq: 1, #new-token: 83, #cached-token: 217, token usage: 0.06, #running-req: 29, #queue-req: 0\n",
      "[2025-08-12 20:41:07] INFO:     127.0.0.1:49656 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:07] INFO:     127.0.0.1:49582 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:07] Prefill batch. #new-seq: 2, #new-token: 157, #cached-token: 423, token usage: 0.05, #running-req: 28, #queue-req: 0\n",
      "[2025-08-12 20:41:07] INFO:     127.0.0.1:49706 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:07] Prefill batch. #new-seq: 1, #new-token: 263, #cached-token: 102, token usage: 0.06, #running-req: 29, #queue-req: 0\n",
      "[2025-08-12 20:41:07] INFO:     127.0.0.1:49740 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:08] Prefill batch. #new-seq: 1, #new-token: 170, #cached-token: 193, token usage: 0.06, #running-req: 29, #queue-req: 0\n",
      "[2025-08-12 20:41:08] INFO:     127.0.0.1:49750 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:08] Prefill batch. #new-seq: 1, #new-token: 173, #cached-token: 194, token usage: 0.06, #running-req: 29, #queue-req: 0\n",
      "[2025-08-12 20:41:09] INFO:     127.0.0.1:49728 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:09] Decode batch. #running-req: 30, #token: 4773, token usage: 0.06, cuda graph: True, gen throughput (token/s): 428.77, #queue-req: 0\n",
      "[2025-08-12 20:41:09] Prefill batch. #new-seq: 1, #new-token: 163, #cached-token: 205, token usage: 0.06, #running-req: 29, #queue-req: 0\n",
      "[2025-08-12 20:41:09] INFO:     127.0.0.1:49702 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:09] Prefill batch. #new-seq: 1, #new-token: 223, #cached-token: 118, token usage: 0.06, #running-req: 29, #queue-req: 0\n",
      "[2025-08-12 20:41:09] INFO:     127.0.0.1:49720 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:09] Prefill batch. #new-seq: 1, #new-token: 144, #cached-token: 221, token usage: 0.07, #running-req: 29, #queue-req: 0\n",
      "[2025-08-12 20:41:09] INFO:     127.0.0.1:49752 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:09] INFO:     127.0.0.1:49878 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:10] Prefill batch. #new-seq: 2, #new-token: 343, #cached-token: 398, token usage: 0.06, #running-req: 28, #queue-req: 0\n",
      "[2025-08-12 20:41:10] INFO:     127.0.0.1:49766 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:10] Prefill batch. #new-seq: 1, #new-token: 125, #cached-token: 121, token usage: 0.07, #running-req: 30, #queue-req: 0\n",
      "[2025-08-12 20:41:10] INFO:     127.0.0.1:49748 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:10] Prefill batch. #new-seq: 1, #new-token: 111, #cached-token: 209, token usage: 0.07, #running-req: 29, #queue-req: 0\n",
      "[2025-08-12 20:41:11] Decode batch. #running-req: 30, #token: 6098, token usage: 0.08, cuda graph: True, gen throughput (token/s): 520.82, #queue-req: 0\n",
      "[2025-08-12 20:41:11] INFO:     127.0.0.1:49788 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:11] Prefill batch. #new-seq: 1, #new-token: 86, #cached-token: 170, token usage: 0.08, #running-req: 29, #queue-req: 0\n",
      "[2025-08-12 20:41:11] INFO:     127.0.0.1:49856 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:11] INFO:     127.0.0.1:49808 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:12] Prefill batch. #new-seq: 2, #new-token: 260, #cached-token: 477, token usage: 0.08, #running-req: 28, #queue-req: 0\n",
      "[2025-08-12 20:41:12] INFO:     127.0.0.1:49894 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:12] Prefill batch. #new-seq: 1, #new-token: 159, #cached-token: 221, token usage: 0.08, #running-req: 29, #queue-req: 0\n",
      "[2025-08-12 20:41:13] INFO:     127.0.0.1:49796 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:13] Prefill batch. #new-seq: 1, #new-token: 98, #cached-token: 234, token usage: 0.09, #running-req: 29, #queue-req: 0\n",
      "[2025-08-12 20:41:13] INFO:     127.0.0.1:49710 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:13] INFO:     127.0.0.1:49840 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:13] Prefill batch. #new-seq: 2, #new-token: 240, #cached-token: 390, token usage: 0.08, #running-req: 28, #queue-req: 0\n",
      "[2025-08-12 20:41:13] INFO:     127.0.0.1:49676 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:13] Prefill batch. #new-seq: 1, #new-token: 84, #cached-token: 172, token usage: 0.08, #running-req: 29, #queue-req: 0\n",
      "[2025-08-12 20:41:13] Decode batch. #running-req: 30, #token: 6505, token usage: 0.09, cuda graph: True, gen throughput (token/s): 521.35, #queue-req: 0\n",
      "[2025-08-12 20:41:13] INFO:     127.0.0.1:49872 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:13] Prefill batch. #new-seq: 1, #new-token: 74, #cached-token: 172, token usage: 0.08, #running-req: 29, #queue-req: 0\n",
      "[2025-08-12 20:41:13] INFO:     127.0.0.1:34666 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:13] Prefill batch. #new-seq: 1, #new-token: 176, #cached-token: 144, token usage: 0.08, #running-req: 29, #queue-req: 0\n",
      "[2025-08-12 20:41:14] INFO:     127.0.0.1:49688 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:14] Prefill batch. #new-seq: 1, #new-token: 93, #cached-token: 172, token usage: 0.08, #running-req: 29, #queue-req: 0\n",
      "[2025-08-12 20:41:14] INFO:     127.0.0.1:34630 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:14] Prefill batch. #new-seq: 1, #new-token: 353, #cached-token: 102, token usage: 0.08, #running-req: 29, #queue-req: 0\n",
      "[2025-08-12 20:41:14] INFO:     127.0.0.1:49792 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:14] Prefill batch. #new-seq: 1, #new-token: 112, #cached-token: 225, token usage: 0.09, #running-req: 30, #queue-req: 0\n",
      "[2025-08-12 20:41:14] INFO:     127.0.0.1:49902 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:14] Prefill batch. #new-seq: 1, #new-token: 88, #cached-token: 243, token usage: 0.08, #running-req: 29, #queue-req: 0\n",
      "[2025-08-12 20:41:14] INFO:     127.0.0.1:34628 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:14] Prefill batch. #new-seq: 1, #new-token: 92, #cached-token: 233, token usage: 0.08, #running-req: 30, #queue-req: 0\n",
      "[2025-08-12 20:41:15] INFO:     127.0.0.1:34662 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:15] INFO:     127.0.0.1:49770 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:15] INFO:     127.0.0.1:34654 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:15] INFO:     127.0.0.1:34714 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:15] Prefill batch. #new-seq: 4, #new-token: 816, #cached-token: 714, token usage: 0.08, #running-req: 26, #queue-req: 0\n",
      "[2025-08-12 20:41:15] INFO:     127.0.0.1:49782 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:15] Prefill batch. #new-seq: 1, #new-token: 96, #cached-token: 169, token usage: 0.09, #running-req: 29, #queue-req: 0\n",
      "[2025-08-12 20:41:16] INFO:     127.0.0.1:34616 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:16] INFO:     127.0.0.1:34670 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:16] INFO:     127.0.0.1:34690 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:16] INFO:     127.0.0.1:34720 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:16] INFO:     127.0.0.1:34646 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:16] Decode batch. #running-req: 26, #token: 5615, token usage: 0.07, cuda graph: True, gen throughput (token/s): 450.02, #queue-req: 0\n",
      "[2025-08-12 20:41:16] Prefill batch. #new-seq: 4, #new-token: 705, #cached-token: 741, token usage: 0.08, #running-req: 25, #queue-req: 0\n",
      "[2025-08-12 20:41:16] Prefill batch. #new-seq: 1, #new-token: 239, #cached-token: 205, token usage: 0.09, #running-req: 29, #queue-req: 0\n",
      "[2025-08-12 20:41:17] INFO:     127.0.0.1:34700 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:17] Prefill batch. #new-seq: 1, #new-token: 165, #cached-token: 145, token usage: 0.09, #running-req: 29, #queue-req: 0\n",
      "[2025-08-12 20:41:17] INFO:     127.0.0.1:49908 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:17] Prefill batch. #new-seq: 1, #new-token: 153, #cached-token: 196, token usage: 0.09, #running-req: 29, #queue-req: 0\n",
      "[2025-08-12 20:41:17] INFO:     127.0.0.1:34728 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:18] Prefill batch. #new-seq: 1, #new-token: 194, #cached-token: 243, token usage: 0.09, #running-req: 29, #queue-req: 0\n",
      "[2025-08-12 20:41:18] INFO:     127.0.0.1:34750 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:18] Decode batch. #running-req: 30, #token: 7339, token usage: 0.10, cuda graph: True, gen throughput (token/s): 495.78, #queue-req: 0\n",
      "[2025-08-12 20:41:18] Prefill batch. #new-seq: 1, #new-token: 172, #cached-token: 146, token usage: 0.10, #running-req: 29, #queue-req: 0\n",
      "[2025-08-12 20:41:19] INFO:     127.0.0.1:49824 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:19] Prefill batch. #new-seq: 1, #new-token: 83, #cached-token: 170, token usage: 0.10, #running-req: 29, #queue-req: 0\n",
      "[2025-08-12 20:41:19] INFO:     127.0.0.1:34758 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:19] INFO:     127.0.0.1:34804 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:19] Prefill batch. #new-seq: 2, #new-token: 392, #cached-token: 291, token usage: 0.10, #running-req: 28, #queue-req: 0\n",
      "[2025-08-12 20:41:20] INFO:     127.0.0.1:34744 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:20] INFO:     127.0.0.1:49858 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:20] INFO:     127.0.0.1:34840 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:20] INFO:     127.0.0.1:34846 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:20] INFO:     127.0.0.1:34734 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:20] Prefill batch. #new-seq: 3, #new-token: 569, #cached-token: 552, token usage: 0.09, #running-req: 25, #queue-req: 0\n",
      "[2025-08-12 20:41:20] Prefill batch. #new-seq: 2, #new-token: 397, #cached-token: 387, token usage: 0.10, #running-req: 28, #queue-req: 0\n",
      "[2025-08-12 20:41:21] Decode batch. #running-req: 30, #token: 7949, token usage: 0.11, cuda graph: True, gen throughput (token/s): 478.60, #queue-req: 0\n",
      "[2025-08-12 20:41:21] INFO:     127.0.0.1:34678 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:21] INFO:     127.0.0.1:34884 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:21] INFO:     127.0.0.1:34764 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:21] INFO:     127.0.0.1:34780 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:21] INFO:     127.0.0.1:34824 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:21] INFO:     127.0.0.1:34774 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:21] INFO:     127.0.0.1:34934 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:21] INFO:     127.0.0.1:34810 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:22] Prefill batch. #new-seq: 1, #new-token: 284, #cached-token: 181, token usage: 0.09, #running-req: 22, #queue-req: 0\n",
      "[2025-08-12 20:41:22] Prefill batch. #new-seq: 7, #new-token: 1481, #cached-token: 981, token usage: 0.09, #running-req: 23, #queue-req: 0\n",
      "[2025-08-12 20:41:23] INFO:     127.0.0.1:60682 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:23] INFO:     127.0.0.1:49744 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:23] Decode batch. #running-req: 29, #token: 7901, token usage: 0.11, cuda graph: True, gen throughput (token/s): 462.37, #queue-req: 0\n",
      "[2025-08-12 20:41:23] Prefill batch. #new-seq: 2, #new-token: 318, #cached-token: 320, token usage: 0.11, #running-req: 28, #queue-req: 0\n",
      "[2025-08-12 20:41:23] INFO:     127.0.0.1:34922 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:23] Prefill batch. #new-seq: 1, #new-token: 323, #cached-token: 193, token usage: 0.11, #running-req: 29, #queue-req: 0\n",
      "[2025-08-12 20:41:23] INFO:     127.0.0.1:34890 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:24] Prefill batch. #new-seq: 1, #new-token: 333, #cached-token: 194, token usage: 0.11, #running-req: 29, #queue-req: 0\n",
      "[2025-08-12 20:41:24] INFO:     127.0.0.1:34908 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:24] Prefill batch. #new-seq: 1, #new-token: 321, #cached-token: 205, token usage: 0.11, #running-req: 29, #queue-req: 0\n",
      "[2025-08-12 20:41:24] INFO:     127.0.0.1:34952 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:25] Prefill batch. #new-seq: 1, #new-token: 307, #cached-token: 217, token usage: 0.11, #running-req: 29, #queue-req: 0\n",
      "[2025-08-12 20:41:25] INFO:     127.0.0.1:34858 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:25] INFO:     127.0.0.1:34954 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:25] Prefill batch. #new-seq: 2, #new-token: 426, #cached-token: 339, token usage: 0.11, #running-req: 28, #queue-req: 0\n",
      "[2025-08-12 20:41:25] INFO:     127.0.0.1:60752 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:26] Decode batch. #running-req: 29, #token: 8676, token usage: 0.12, cuda graph: True, gen throughput (token/s): 465.61, #queue-req: 0\n",
      "[2025-08-12 20:41:26] Prefill batch. #new-seq: 1, #new-token: 176, #cached-token: 149, token usage: 0.12, #running-req: 29, #queue-req: 0\n",
      "[2025-08-12 20:41:26] INFO:     127.0.0.1:34868 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:26] Prefill batch. #new-seq: 1, #new-token: 275, #cached-token: 121, token usage: 0.11, #running-req: 29, #queue-req: 0\n",
      "[2025-08-12 20:41:27] INFO:     127.0.0.1:60706 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:27] INFO:     127.0.0.1:60696 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:27] Prefill batch. #new-seq: 1, #new-token: 178, #cached-token: 145, token usage: 0.12, #running-req: 29, #queue-req: 0\n",
      "[2025-08-12 20:41:27] Prefill batch. #new-seq: 1, #new-token: 271, #cached-token: 243, token usage: 0.12, #running-req: 29, #queue-req: 0\n",
      "[2025-08-12 20:41:27] INFO:     127.0.0.1:34936 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:27] Prefill batch. #new-seq: 1, #new-token: 88, #cached-token: 170, token usage: 0.12, #running-req: 29, #queue-req: 0\n",
      "[2025-08-12 20:41:28] Decode batch. #running-req: 30, #token: 9594, token usage: 0.13, cuda graph: True, gen throughput (token/s): 455.35, #queue-req: 0\n",
      "[2025-08-12 20:41:28] INFO:     127.0.0.1:60772 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:28] Prefill batch. #new-seq: 1, #new-token: 312, #cached-token: 234, token usage: 0.12, #running-req: 29, #queue-req: 0\n",
      "[2025-08-12 20:41:28] INFO:     127.0.0.1:60722 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:29] Prefill batch. #new-seq: 1, #new-token: 322, #cached-token: 221, token usage: 0.12, #running-req: 29, #queue-req: 0\n",
      "[2025-08-12 20:41:30] INFO:     127.0.0.1:60736 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:30] Prefill batch. #new-seq: 1, #new-token: 163, #cached-token: 242, token usage: 0.14, #running-req: 29, #queue-req: 0\n",
      "[2025-08-12 20:41:30] INFO:     127.0.0.1:34902 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:30] Decode batch. #running-req: 29, #token: 10102, token usage: 0.13, cuda graph: True, gen throughput (token/s): 545.63, #queue-req: 0\n",
      "[2025-08-12 20:41:30] Prefill batch. #new-seq: 1, #new-token: 181, #cached-token: 145, token usage: 0.13, #running-req: 29, #queue-req: 0\n",
      "[2025-08-12 20:41:31] INFO:     127.0.0.1:60720 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:31] Prefill batch. #new-seq: 1, #new-token: 249, #cached-token: 220, token usage: 0.13, #running-req: 29, #queue-req: 0\n",
      "[2025-08-12 20:41:31] INFO:     127.0.0.1:60906 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:31] Prefill batch. #new-seq: 1, #new-token: 505, #cached-token: 102, token usage: 0.13, #running-req: 29, #queue-req: 0\n",
      "[2025-08-12 20:41:31] INFO:     127.0.0.1:34788 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:31] INFO:     127.0.0.1:60698 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:31] INFO:     127.0.0.1:60942 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:31] Prefill batch. #new-seq: 3, #new-token: 593, #cached-token: 590, token usage: 0.13, #running-req: 27, #queue-req: 0\n",
      "[2025-08-12 20:41:32] INFO:     127.0.0.1:60860 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:32] INFO:     127.0.0.1:60896 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:32] Prefill batch. #new-seq: 2, #new-token: 830, #cached-token: 387, token usage: 0.13, #running-req: 28, #queue-req: 0\n",
      "[2025-08-12 20:41:33] INFO:     127.0.0.1:60768 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:33] INFO:     127.0.0.1:60832 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:33] Prefill batch. #new-seq: 1, #new-token: 217, #cached-token: 233, token usage: 0.13, #running-req: 29, #queue-req: 0\n",
      "[2025-08-12 20:41:33] Prefill batch. #new-seq: 1, #new-token: 110, #cached-token: 169, token usage: 0.14, #running-req: 30, #queue-req: 0\n",
      "[2025-08-12 20:41:33] Decode batch. #running-req: 30, #token: 10075, token usage: 0.13, cuda graph: True, gen throughput (token/s): 409.30, #queue-req: 0\n",
      "[2025-08-12 20:41:33] INFO:     127.0.0.1:60816 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:33] Prefill batch. #new-seq: 1, #new-token: 88, #cached-token: 172, token usage: 0.13, #running-req: 29, #queue-req: 0\n",
      "[2025-08-12 20:41:33] INFO:     127.0.0.1:60800 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:33] INFO:     127.0.0.1:60848 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:33] Prefill batch. #new-seq: 2, #new-token: 198, #cached-token: 342, token usage: 0.13, #running-req: 30, #queue-req: 0\n",
      "[2025-08-12 20:41:33] INFO:     127.0.0.1:54254 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:34] INFO:     127.0.0.1:60922 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:34] Prefill batch. #new-seq: 1, #new-token: 162, #cached-token: 160, token usage: 0.12, #running-req: 29, #queue-req: 0\n",
      "[2025-08-12 20:41:34] Prefill batch. #new-seq: 1, #new-token: 365, #cached-token: 205, token usage: 0.13, #running-req: 30, #queue-req: 0\n",
      "[2025-08-12 20:41:34] INFO:     127.0.0.1:60710 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:34] Prefill batch. #new-seq: 1, #new-token: 107, #cached-token: 172, token usage: 0.13, #running-req: 29, #queue-req: 0\n",
      "[2025-08-12 20:41:34] INFO:     127.0.0.1:60786 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:34] INFO:     127.0.0.1:60874 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:35] Prefill batch. #new-seq: 2, #new-token: 608, #cached-token: 377, token usage: 0.12, #running-req: 28, #queue-req: 0\n",
      "[2025-08-12 20:41:35] INFO:     127.0.0.1:60788 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:35] INFO:     127.0.0.1:54284 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:35] Prefill batch. #new-seq: 2, #new-token: 449, #cached-token: 338, token usage: 0.13, #running-req: 28, #queue-req: 0\n",
      "[2025-08-12 20:41:36] INFO:     127.0.0.1:60962 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:36] Decode batch. #running-req: 30, #token: 9601, token usage: 0.13, cuda graph: True, gen throughput (token/s): 455.58, #queue-req: 0\n",
      "[2025-08-12 20:41:36] Prefill batch. #new-seq: 1, #new-token: 352, #cached-token: 243, token usage: 0.13, #running-req: 29, #queue-req: 0\n",
      "[2025-08-12 20:41:36] INFO:     127.0.0.1:54266 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:36] INFO:     127.0.0.1:60908 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:36] Prefill batch. #new-seq: 2, #new-token: 806, #cached-token: 442, token usage: 0.12, #running-req: 28, #queue-req: 0\n",
      "[2025-08-12 20:41:36] INFO:     127.0.0.1:60916 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:37] Prefill batch. #new-seq: 1, #new-token: 379, #cached-token: 217, token usage: 0.13, #running-req: 29, #queue-req: 0\n",
      "[2025-08-12 20:41:37] INFO:     127.0.0.1:60886 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:37] Prefill batch. #new-seq: 1, #new-token: 232, #cached-token: 152, token usage: 0.13, #running-req: 29, #queue-req: 0\n",
      "[2025-08-12 20:41:37] INFO:     127.0.0.1:54344 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:37] Prefill batch. #new-seq: 1, #new-token: 190, #cached-token: 156, token usage: 0.13, #running-req: 29, #queue-req: 0\n",
      "[2025-08-12 20:41:38] INFO:     127.0.0.1:60936 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:38] Prefill batch. #new-seq: 1, #new-token: 254, #cached-token: 185, token usage: 0.13, #running-req: 29, #queue-req: 0\n",
      "[2025-08-12 20:41:38] INFO:     127.0.0.1:54302 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:38] Prefill batch. #new-seq: 1, #new-token: 221, #cached-token: 157, token usage: 0.13, #running-req: 29, #queue-req: 0\n",
      "[2025-08-12 20:41:38] Decode batch. #running-req: 29, #token: 10314, token usage: 0.14, cuda graph: True, gen throughput (token/s): 453.20, #queue-req: 0\n",
      "[2025-08-12 20:41:39] INFO:     127.0.0.1:54340 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:39] Prefill batch. #new-seq: 1, #new-token: 359, #cached-token: 118, token usage: 0.13, #running-req: 29, #queue-req: 0\n",
      "[2025-08-12 20:41:39] INFO:     127.0.0.1:54390 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:39] INFO:     127.0.0.1:60956 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:39] Prefill batch. #new-seq: 2, #new-token: 291, #cached-token: 321, token usage: 0.13, #running-req: 28, #queue-req: 0\n",
      "[2025-08-12 20:41:40] INFO:     127.0.0.1:54358 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      ".[2025-08-12 20:41:40] INFO:     127.0.0.1:54310 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      ".[2025-08-12 20:41:40] INFO:     127.0.0.1:54282 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:40] INFO:     127.0.0.1:54432 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:40] INFO:     127.0.0.1:54360 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:40] Prefill batch. #new-seq: 3, #new-token: 785, #cached-token: 517, token usage: 0.11, #running-req: 25, #queue-req: 0\n",
      "[2025-08-12 20:41:40] INFO:     127.0.0.1:54326 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:40] INFO:     127.0.0.1:60928 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:41] Prefill batch. #new-seq: 2, #new-token: 393, #cached-token: 302, token usage: 0.12, #running-req: 26, #queue-req: 0\n",
      "[2025-08-12 20:41:41] Decode batch. #running-req: 28, #token: 9520, token usage: 0.13, cuda graph: True, gen throughput (token/s): 453.28, #queue-req: 0\n",
      "[2025-08-12 20:41:41] INFO:     127.0.0.1:54350 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:41] INFO:     127.0.0.1:54374 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      ".[2025-08-12 20:41:42] Prefill batch. #new-seq: 1, #new-token: 180, #cached-token: 151, token usage: 0.12, #running-req: 26, #queue-req: 0\n",
      "[2025-08-12 20:41:42] INFO:     127.0.0.1:54372 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:42] Prefill batch. #new-seq: 1, #new-token: 188, #cached-token: 155, token usage: 0.12, #running-req: 26, #queue-req: 0\n",
      "[2025-08-12 20:41:42] INFO:     127.0.0.1:54394 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:42] INFO:     127.0.0.1:54294 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:42] Prefill batch. #new-seq: 2, #new-token: 301, #cached-token: 318, token usage: 0.12, #running-req: 25, #queue-req: 0\n",
      "[2025-08-12 20:41:43] INFO:     127.0.0.1:54464 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      ".[2025-08-12 20:41:43] Decode batch. #running-req: 26, #token: 9182, token usage: 0.12, cuda graph: True, gen throughput (token/s): 498.42, #queue-req: 0\n",
      "[2025-08-12 20:41:43] INFO:     127.0.0.1:60824 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:43] Prefill batch. #new-seq: 1, #new-token: 213, #cached-token: 225, token usage: 0.12, #running-req: 25, #queue-req: 0\n",
      "[2025-08-12 20:41:45] INFO:     127.0.0.1:54428 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:45] INFO:     127.0.0.1:35226 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:45] INFO:     127.0.0.1:54484 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:45] INFO:     127.0.0.1:35246 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      ".[2025-08-12 20:41:45] Decode batch. #running-req: 22, #token: 7017, token usage: 0.09, cuda graph: True, gen throughput (token/s): 514.68, #queue-req: 0\n",
      "[2025-08-12 20:41:45] Prefill batch. #new-seq: 1, #new-token: 452, #cached-token: 181, token usage: 0.09, #running-req: 22, #queue-req: 0\n",
      "[2025-08-12 20:41:45] INFO:     127.0.0.1:54408 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:45] INFO:     127.0.0.1:54460 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:45] Prefill batch. #new-seq: 1, #new-token: 197, #cached-token: 151, token usage: 0.09, #running-req: 23, #queue-req: 0\n",
      "[2025-08-12 20:41:45] INFO:     127.0.0.1:54468 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "..[2025-08-12 20:41:45] Prefill batch. #new-seq: 2, #new-token: 302, #cached-token: 290, token usage: 0.10, #running-req: 21, #queue-req: 0\n",
      "[2025-08-12 20:41:46] INFO:     127.0.0.1:54414 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:46] INFO:     127.0.0.1:35256 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      ".[2025-08-12 20:41:46] Prefill batch. #new-seq: 1, #new-token: 192, #cached-token: 152, token usage: 0.09, #running-req: 21, #queue-req: 0\n",
      "[2025-08-12 20:41:46] INFO:     127.0.0.1:54426 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:46] Prefill batch. #new-seq: 1, #new-token: 192, #cached-token: 157, token usage: 0.09, #running-req: 21, #queue-req: 0\n",
      "[2025-08-12 20:41:47] Decode batch. #running-req: 22, #token: 7629, token usage: 0.10, cuda graph: True, gen throughput (token/s): 366.74, #queue-req: 0\n",
      "[2025-08-12 20:41:48] INFO:     127.0.0.1:35272 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:48] INFO:     127.0.0.1:54444 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      ".[2025-08-12 20:41:48] Prefill batch. #new-seq: 1, #new-token: 223, #cached-token: 151, token usage: 0.09, #running-req: 20, #queue-req: 0\n",
      "[2025-08-12 20:41:49] INFO:     127.0.0.1:35312 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:49] Prefill batch. #new-seq: 1, #new-token: 154, #cached-token: 179, token usage: 0.10, #running-req: 20, #queue-req: 0\n",
      "[2025-08-12 20:41:49] INFO:     127.0.0.1:35218 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:49] Prefill batch. #new-seq: 1, #new-token: 294, #cached-token: 234, token usage: 0.10, #running-req: 20, #queue-req: 0\n",
      "[2025-08-12 20:41:50] Decode batch. #running-req: 20, #token: 7488, token usage: 0.10, cuda graph: True, gen throughput (token/s): 399.76, #queue-req: 0\n",
      "[2025-08-12 20:41:50] INFO:     127.0.0.1:35230 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:50] Prefill batch. #new-seq: 1, #new-token: 332, #cached-token: 220, token usage: 0.10, #running-req: 20, #queue-req: 0\n",
      "[2025-08-12 20:41:50] INFO:     127.0.0.1:35320 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:51] Prefill batch. #new-seq: 1, #new-token: 197, #cached-token: 148, token usage: 0.10, #running-req: 20, #queue-req: 0\n",
      "[2025-08-12 20:41:52] Decode batch. #running-req: 21, #token: 7969, token usage: 0.11, cuda graph: True, gen throughput (token/s): 381.80, #queue-req: 0\n",
      "[2025-08-12 20:41:53] INFO:     127.0.0.1:35288 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:53] INFO:     127.0.0.1:35274 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:53] Prefill batch. #new-seq: 1, #new-token: 123, #cached-token: 170, token usage: 0.10, #running-req: 20, #queue-req: 0\n",
      "[2025-08-12 20:41:53] Prefill batch. #new-seq: 1, #new-token: 347, #cached-token: 196, token usage: 0.10, #running-req: 21, #queue-req: 0\n",
      "[2025-08-12 20:41:53] INFO:     127.0.0.1:35278 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:53] Prefill batch. #new-seq: 1, #new-token: 298, #cached-token: 233, token usage: 0.10, #running-req: 20, #queue-req: 0\n",
      "[2025-08-12 20:41:54] Decode batch. #running-req: 21, #token: 8311, token usage: 0.11, cuda graph: True, gen throughput (token/s): 381.09, #queue-req: 0\n",
      "[2025-08-12 20:41:54] INFO:     127.0.0.1:35328 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:54] INFO:     127.0.0.1:35366 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      ".[2025-08-12 20:41:55] Prefill batch. #new-seq: 1, #new-token: 184, #cached-token: 148, token usage: 0.09, #running-req: 19, #queue-req: 0\n",
      "[2025-08-12 20:41:55] INFO:     127.0.0.1:35250 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:55] Prefill batch. #new-seq: 1, #new-token: 120, #cached-token: 173, token usage: 0.10, #running-req: 20, #queue-req: 0\n",
      "[2025-08-12 20:41:55] INFO:     127.0.0.1:35308 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:55] Prefill batch. #new-seq: 1, #new-token: 102, #cached-token: 169, token usage: 0.09, #running-req: 19, #queue-req: 0\n",
      "[2025-08-12 20:41:55] INFO:     127.0.0.1:35398 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:55] Prefill batch. #new-seq: 1, #new-token: 95, #cached-token: 172, token usage: 0.09, #running-req: 20, #queue-req: 0\n",
      "[2025-08-12 20:41:56] INFO:     127.0.0.1:35296 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:56] Prefill batch. #new-seq: 1, #new-token: 102, #cached-token: 172, token usage: 0.09, #running-req: 19, #queue-req: 0\n",
      "[2025-08-12 20:41:56] INFO:     127.0.0.1:48340 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:56] Prefill batch. #new-seq: 1, #new-token: 185, #cached-token: 179, token usage: 0.09, #running-req: 19, #queue-req: 0\n",
      "[2025-08-12 20:41:56] Decode batch. #running-req: 20, #token: 6862, token usage: 0.09, cuda graph: True, gen throughput (token/s): 348.47, #queue-req: 0\n",
      "[2025-08-12 20:41:56] INFO:     127.0.0.1:54440 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:56] INFO:     127.0.0.1:35350 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:56] Prefill batch. #new-seq: 2, #new-token: 787, #cached-token: 360, token usage: 0.08, #running-req: 18, #queue-req: 0\n",
      "[2025-08-12 20:41:57] INFO:     127.0.0.1:48352 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:57] INFO:     127.0.0.1:35280 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:57] Prefill batch. #new-seq: 2, #new-token: 547, #cached-token: 372, token usage: 0.08, #running-req: 18, #queue-req: 0\n",
      "[2025-08-12 20:41:58] INFO:     127.0.0.1:48394 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:58] Prefill batch. #new-seq: 1, #new-token: 198, #cached-token: 172, token usage: 0.09, #running-req: 19, #queue-req: 0\n",
      "[2025-08-12 20:41:59] Decode batch. #running-req: 20, #token: 6946, token usage: 0.09, cuda graph: True, gen throughput (token/s): 332.38, #queue-req: 0\n",
      "[2025-08-12 20:41:59] INFO:     127.0.0.1:48332 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:59] Prefill batch. #new-seq: 1, #new-token: 98, #cached-token: 170, token usage: 0.09, #running-req: 19, #queue-req: 0\n",
      "[2025-08-12 20:41:59] INFO:     127.0.0.1:35334 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:59] Prefill batch. #new-seq: 1, #new-token: 324, #cached-token: 185, token usage: 0.08, #running-req: 19, #queue-req: 0\n",
      "[2025-08-12 20:41:59] INFO:     127.0.0.1:35210 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:59] Prefill batch. #new-seq: 1, #new-token: 182, #cached-token: 169, token usage: 0.08, #running-req: 20, #queue-req: 0\n",
      "[2025-08-12 20:41:59] INFO:     127.0.0.1:35298 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:41:59] Prefill batch. #new-seq: 1, #new-token: 174, #cached-token: 170, token usage: 0.08, #running-req: 19, #queue-req: 0\n",
      "[2025-08-12 20:42:00] INFO:     127.0.0.1:48368 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:42:00] Prefill batch. #new-seq: 1, #new-token: 289, #cached-token: 225, token usage: 0.08, #running-req: 19, #queue-req: 0\n",
      "[2025-08-12 20:42:01] Decode batch. #running-req: 20, #token: 6578, token usage: 0.09, cuda graph: True, gen throughput (token/s): 344.11, #queue-req: 0\n",
      "[2025-08-12 20:42:02] INFO:     127.0.0.1:48402 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:42:02] Prefill batch. #new-seq: 1, #new-token: 202, #cached-token: 156, token usage: 0.09, #running-req: 19, #queue-req: 0\n",
      "[2025-08-12 20:42:02] INFO:     127.0.0.1:48452 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:42:02] INFO:     127.0.0.1:48466 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:42:02] Prefill batch. #new-seq: 2, #new-token: 417, #cached-token: 329, token usage: 0.08, #running-req: 18, #queue-req: 0\n",
      "[2025-08-12 20:42:03] INFO:     127.0.0.1:48320 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:42:03] Prefill batch. #new-seq: 1, #new-token: 358, #cached-token: 184, token usage: 0.08, #running-req: 19, #queue-req: 0\n",
      "[2025-08-12 20:42:03] Decode batch. #running-req: 20, #token: 6422, token usage: 0.09, cuda graph: True, gen throughput (token/s): 354.76, #queue-req: 0\n",
      "[2025-08-12 20:42:03] INFO:     127.0.0.1:48384 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:42:04] Prefill batch. #new-seq: 1, #new-token: 212, #cached-token: 156, token usage: 0.08, #running-req: 19, #queue-req: 0\n",
      "[2025-08-12 20:42:04] INFO:     127.0.0.1:48414 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:42:04] Prefill batch. #new-seq: 1, #new-token: 209, #cached-token: 154, token usage: 0.08, #running-req: 19, #queue-req: 0\n",
      "[2025-08-12 20:42:04] INFO:     127.0.0.1:48434 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:42:04] Prefill batch. #new-seq: 1, #new-token: 178, #cached-token: 167, token usage: 0.08, #running-req: 19, #queue-req: 0\n",
      "[2025-08-12 20:42:04] INFO:     127.0.0.1:35382 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:42:04] Prefill batch. #new-seq: 1, #new-token: 115, #cached-token: 171, token usage: 0.08, #running-req: 19, #queue-req: 0\n",
      "[2025-08-12 20:42:04] INFO:     127.0.0.1:57710 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:42:05] Prefill batch. #new-seq: 1, #new-token: 190, #cached-token: 146, token usage: 0.08, #running-req: 19, #queue-req: 0\n",
      "[2025-08-12 20:42:05] INFO:     127.0.0.1:57712 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:42:05] Prefill batch. #new-seq: 1, #new-token: 175, #cached-token: 147, token usage: 0.08, #running-req: 19, #queue-req: 0\n",
      "[2025-08-12 20:42:05] INFO:     127.0.0.1:48426 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:42:05] Decode batch. #running-req: 19, #token: 6072, token usage: 0.08, cuda graph: True, gen throughput (token/s): 335.66, #queue-req: 0\n",
      "[2025-08-12 20:42:05] Prefill batch. #new-seq: 1, #new-token: 180, #cached-token: 159, token usage: 0.08, #running-req: 19, #queue-req: 0\n",
      "[2025-08-12 20:42:06] INFO:     127.0.0.1:57684 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:42:06] Prefill batch. #new-seq: 1, #new-token: 212, #cached-token: 159, token usage: 0.08, #running-req: 19, #queue-req: 0\n",
      "[2025-08-12 20:42:06] INFO:     127.0.0.1:48428 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:42:06] Prefill batch. #new-seq: 1, #new-token: 114, #cached-token: 150, token usage: 0.08, #running-req: 19, #queue-req: 0\n",
      "[2025-08-12 20:42:08] Decode batch. #running-req: 20, #token: 6662, token usage: 0.09, cuda graph: True, gen throughput (token/s): 354.03, #queue-req: 0\n",
      "[2025-08-12 20:42:08] INFO:     127.0.0.1:57744 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:42:08] INFO:     127.0.0.1:48442 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:42:08] Prefill batch. #new-seq: 2, #new-token: 597, #cached-token: 383, token usage: 0.08, #running-req: 18, #queue-req: 0\n",
      "[2025-08-12 20:42:10] INFO:     127.0.0.1:57736 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:42:10] Prefill batch. #new-seq: 1, #new-token: 263, #cached-token: 145, token usage: 0.09, #running-req: 19, #queue-req: 0\n",
      "[2025-08-12 20:42:10] Decode batch. #running-req: 20, #token: 7183, token usage: 0.10, cuda graph: True, gen throughput (token/s): 361.76, #queue-req: 0\n",
      "[2025-08-12 20:42:10] INFO:     127.0.0.1:57682 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:42:10] Prefill batch. #new-seq: 1, #new-token: 413, #cached-token: 220, token usage: 0.09, #running-req: 19, #queue-req: 0\n",
      "[2025-08-12 20:42:11] INFO:     127.0.0.1:57766 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:42:11] Prefill batch. #new-seq: 1, #new-token: 225, #cached-token: 149, token usage: 0.09, #running-req: 19, #queue-req: 0\n",
      "[2025-08-12 20:42:12] INFO:     127.0.0.1:57876 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      ".[2025-08-12 20:42:12] Decode batch. #running-req: 20, #token: 7370, token usage: 0.10, cuda graph: True, gen throughput (token/s): 374.21, #queue-req: 0\n",
      "[2025-08-12 20:42:13] INFO:     127.0.0.1:57726 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:42:13] Prefill batch. #new-seq: 1, #new-token: 271, #cached-token: 144, token usage: 0.10, #running-req: 18, #queue-req: 0\n",
      "[2025-08-12 20:42:14] INFO:     127.0.0.1:51326 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:42:14] Prefill batch. #new-seq: 1, #new-token: 250, #cached-token: 147, token usage: 0.10, #running-req: 18, #queue-req: 0\n",
      "[2025-08-12 20:42:14] INFO:     127.0.0.1:57802 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:42:14] Prefill batch. #new-seq: 1, #new-token: 162, #cached-token: 121, token usage: 0.10, #running-req: 18, #queue-req: 0\n",
      "[2025-08-12 20:42:14] Decode batch. #running-req: 18, #token: 7381, token usage: 0.10, cuda graph: True, gen throughput (token/s): 351.50, #queue-req: 0\n",
      "[2025-08-12 20:42:14] INFO:     127.0.0.1:57832 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:42:14] Prefill batch. #new-seq: 1, #new-token: 418, #cached-token: 185, token usage: 0.09, #running-req: 18, #queue-req: 0\n",
      "[2025-08-12 20:42:14] INFO:     127.0.0.1:57776 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:42:15] Prefill batch. #new-seq: 1, #new-token: 109, #cached-token: 170, token usage: 0.10, #running-req: 19, #queue-req: 0\n",
      "[2025-08-12 20:42:16] INFO:     127.0.0.1:57756 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:42:16] Prefill batch. #new-seq: 1, #new-token: 379, #cached-token: 242, token usage: 0.10, #running-req: 18, #queue-req: 0\n",
      "[2025-08-12 20:42:16] INFO:     127.0.0.1:57694 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:42:16] Prefill batch. #new-seq: 1, #new-token: 464, #cached-token: 196, token usage: 0.09, #running-req: 18, #queue-req: 0\n",
      "[2025-08-12 20:42:17] Decode batch. #running-req: 19, #token: 7748, token usage: 0.10, cuda graph: True, gen throughput (token/s): 314.19, #queue-req: 0\n",
      "[2025-08-12 20:42:17] INFO:     127.0.0.1:57818 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:42:17] Prefill batch. #new-seq: 1, #new-token: 118, #cached-token: 171, token usage: 0.09, #running-req: 18, #queue-req: 0\n",
      "[2025-08-12 20:42:17] INFO:     127.0.0.1:57848 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:42:17] Prefill batch. #new-seq: 1, #new-token: 137, #cached-token: 169, token usage: 0.10, #running-req: 19, #queue-req: 0\n",
      "[2025-08-12 20:42:17] INFO:     127.0.0.1:57760 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:42:17] Prefill batch. #new-seq: 1, #new-token: 578, #cached-token: 118, token usage: 0.09, #running-req: 18, #queue-req: 0\n",
      "[2025-08-12 20:42:18] INFO:     127.0.0.1:51322 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:42:18] Prefill batch. #new-seq: 1, #new-token: 393, #cached-token: 209, token usage: 0.09, #running-req: 18, #queue-req: 0\n",
      "[2025-08-12 20:42:18] INFO:     127.0.0.1:57814 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:42:18] Prefill batch. #new-seq: 1, #new-token: 212, #cached-token: 148, token usage: 0.10, #running-req: 18, #queue-req: 0\n",
      "[2025-08-12 20:42:19] INFO:     127.0.0.1:51356 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:42:19] Prefill batch. #new-seq: 1, #new-token: 217, #cached-token: 165, token usage: 0.10, #running-req: 18, #queue-req: 0\n",
      "[2025-08-12 20:42:19] Decode batch. #running-req: 19, #token: 7577, token usage: 0.10, cuda graph: True, gen throughput (token/s): 286.42, #queue-req: 0\n",
      "[2025-08-12 20:42:19] INFO:     127.0.0.1:57786 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:42:19] Prefill batch. #new-seq: 1, #new-token: 127, #cached-token: 172, token usage: 0.10, #running-req: 18, #queue-req: 0\n",
      "[2025-08-12 20:42:20] INFO:     127.0.0.1:57752 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:42:20] Prefill batch. #new-seq: 1, #new-token: 436, #cached-token: 233, token usage: 0.09, #running-req: 18, #queue-req: 0\n",
      "[2025-08-12 20:42:21] INFO:     127.0.0.1:51398 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:42:21] INFO:     127.0.0.1:48412 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:42:21] Prefill batch. #new-seq: 2, #new-token: 295, #cached-token: 323, token usage: 0.09, #running-req: 17, #queue-req: 0\n",
      "[2025-08-12 20:42:21] Decode batch. #running-req: 19, #token: 7011, token usage: 0.09, cuda graph: True, gen throughput (token/s): 340.55, #queue-req: 0\n",
      "[2025-08-12 20:42:22] INFO:     127.0.0.1:51340 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:42:22] Prefill batch. #new-seq: 1, #new-token: 178, #cached-token: 170, token usage: 0.09, #running-req: 18, #queue-req: 0\n",
      "[2025-08-12 20:42:23] INFO:     127.0.0.1:51358 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:42:23] Prefill batch. #new-seq: 1, #new-token: 456, #cached-token: 184, token usage: 0.09, #running-req: 18, #queue-req: 0\n",
      "[2025-08-12 20:42:23] INFO:     127.0.0.1:51408 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:42:24] Decode batch. #running-req: 18, #token: 6426, token usage: 0.09, cuda graph: True, gen throughput (token/s): 355.35, #queue-req: 0\n",
      "[2025-08-12 20:42:24] INFO:     127.0.0.1:51386 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:42:24] INFO:     127.0.0.1:51404 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:42:24] Prefill batch. #new-seq: 3, #new-token: 624, #cached-token: 464, token usage: 0.09, #running-req: 16, #queue-req: 0\n",
      "[2025-08-12 20:42:24] INFO:     127.0.0.1:60884 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:42:24] Prefill batch. #new-seq: 1, #new-token: 232, #cached-token: 150, token usage: 0.09, #running-req: 18, #queue-req: 0\n",
      "[2025-08-12 20:42:24] INFO:     127.0.0.1:57864 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:42:24] Prefill batch. #new-seq: 1, #new-token: 439, #cached-token: 225, token usage: 0.08, #running-req: 18, #queue-req: 0\n",
      "[2025-08-12 20:42:25] INFO:     127.0.0.1:51422 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:42:25] Prefill batch. #new-seq: 1, #new-token: 181, #cached-token: 149, token usage: 0.09, #running-req: 18, #queue-req: 0\n",
      "[2025-08-12 20:42:25] INFO:     127.0.0.1:51420 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:42:25] Prefill batch. #new-seq: 1, #new-token: 231, #cached-token: 171, token usage: 0.08, #running-req: 18, #queue-req: 0\n",
      "[2025-08-12 20:42:26] Decode batch. #running-req: 19, #token: 6782, token usage: 0.09, cuda graph: True, gen throughput (token/s): 300.72, #queue-req: 0\n",
      "[2025-08-12 20:42:27] INFO:     127.0.0.1:60940 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:42:27] Prefill batch. #new-seq: 1, #new-token: 203, #cached-token: 146, token usage: 0.09, #running-req: 18, #queue-req: 0\n",
      "[2025-08-12 20:42:28] Decode batch. #running-req: 19, #token: 7511, token usage: 0.10, cuda graph: True, gen throughput (token/s): 379.62, #queue-req: 0\n",
      "[2025-08-12 20:42:28] INFO:     127.0.0.1:60922 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:42:28] Prefill batch. #new-seq: 1, #new-token: 266, #cached-token: 146, token usage: 0.09, #running-req: 18, #queue-req: 0\n",
      "[2025-08-12 20:42:28] INFO:     127.0.0.1:60896 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:42:28] Prefill batch. #new-seq: 1, #new-token: 108, #cached-token: 173, token usage: 0.09, #running-req: 18, #queue-req: 0\n",
      "[2025-08-12 20:42:29] INFO:     127.0.0.1:60880 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:42:29] INFO:     127.0.0.1:60932 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:42:29] Prefill batch. #new-seq: 2, #new-token: 705, #cached-token: 353, token usage: 0.09, #running-req: 17, #queue-req: 0\n",
      "[2025-08-12 20:42:30] INFO:     127.0.0.1:51374 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:42:30] Prefill batch. #new-seq: 1, #new-token: 507, #cached-token: 234, token usage: 0.09, #running-req: 18, #queue-req: 0\n",
      "[2025-08-12 20:42:31] Decode batch. #running-req: 19, #token: 7805, token usage: 0.10, cuda graph: True, gen throughput (token/s): 278.56, #queue-req: 0\n",
      "[2025-08-12 20:42:32] INFO:     127.0.0.1:60908 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:42:32] INFO:     127.0.0.1:58470 - \"POST /v1/chat/completions HTTP/1.1\" 400 Bad Request\n",
      "> \u001b[0;32m/nas/ucb/dayan/optimal-explorer-dev/src/optimal_explorer/llm_utils.py\u001b[0m(85)\u001b[0;36mllm_call\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     84 \u001b[0;31m                        \u001b[0;32mimport\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 85 \u001b[0;31m                        \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"API request failed: status \"\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     86 \u001b[0;31m                        \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "[2025-08-12 20:42:32] INFO:     127.0.0.1:60954 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:42:32] INFO:     127.0.0.1:60894 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:42:32] INFO:     127.0.0.1:60918 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:42:32] INFO:     127.0.0.1:51368 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:42:33] Decode batch. #running-req: 14, #token: 5528, token usage: 0.07, cuda graph: True, gen throughput (token/s): 373.03, #queue-req: 0\n",
      "[2025-08-12 20:42:33] INFO:     127.0.0.1:32784 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:42:33] INFO:     127.0.0.1:32776 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:42:34] INFO:     127.0.0.1:58464 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:42:34] INFO:     127.0.0.1:60948 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:42:34] Decode batch. #running-req: 10, #token: 4311, token usage: 0.06, cuda graph: True, gen throughput (token/s): 264.40, #queue-req: 0\n",
      "[2025-08-12 20:42:35] INFO:     127.0.0.1:60992 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:42:36] Decode batch. #running-req: 9, #token: 3807, token usage: 0.05, cuda graph: True, gen throughput (token/s): 213.51, #queue-req: 0\n",
      "[2025-08-12 20:42:36] INFO:     127.0.0.1:32774 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:42:36] INFO:     127.0.0.1:58452 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:42:36] INFO:     127.0.0.1:32786 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:42:37] INFO:     127.0.0.1:58468 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:42:38] INFO:     127.0.0.1:58428 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:42:38] Decode batch. #running-req: 4, #token: 2010, token usage: 0.03, cuda graph: True, gen throughput (token/s): 124.81, #queue-req: 0\n",
      "[2025-08-12 20:42:39] INFO:     127.0.0.1:60976 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:42:40] Decode batch. #running-req: 3, #token: 1646, token usage: 0.02, cuda graph: True, gen throughput (token/s): 77.47, #queue-req: 0\n",
      "[2025-08-12 20:42:40] INFO:     127.0.0.1:58442 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:42:40] INFO:     127.0.0.1:60962 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:42:41] Decode batch. #running-req: 1, #token: 628, token usage: 0.01, cuda graph: True, gen throughput (token/s): 39.71, #queue-req: 0\n",
      "[2025-08-12 20:42:42] INFO:     127.0.0.1:58458 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "<ClientResponse(http://localhost:36358/v1/chat/completions) [400 Bad Request]>\n",
      "<CIMultiDictProxy('Date': 'Wed, 13 Aug 2025 03:42:31 GMT', 'Server': 'uvicorn', 'Content-Length': '372', 'Content-Type': 'application/json')>\n",
      "\n",
      "{'method': 'POST', '_real_url': URL('http://localhost:36358/v1/chat/completions'), '_url': URL('http://localhost:36358/v1/chat/completions'), '_ClientResponse__writer': None, '_request_info': RequestInfo(url=URL('http://localhost:36358/v1/chat/completions'), method='POST', headers=<CIMultiDictProxy('Host': 'localhost:36358', 'Accept': '*/*', 'Accept-Encoding': 'gzip, deflate', 'User-Agent': 'Python/3.10 aiohttp/3.12.14', 'Content-Length': '3729', 'Content-Type': 'application/json')>, real_url=URL('http://localhost:36358/v1/chat/completions')), '_timer': <aiohttp.helpers.TimerContext object at 0x7f809ccc4900>, '_cache': {'headers': <CIMultiDictProxy('Date': 'Wed, 13 Aug 2025 03:42:31 GMT', 'Server': 'uvicorn', 'Content-Length': '372', 'Content-Type': 'application/json')>, 'url': URL('http://localhost:36358/v1/chat/completions')}, '_traces': [], '_loop': <_UnixSelectorEventLoop running=True closed=False debug=False>, '_session': None, '_resolve_charset': <function ClientSession.<lambda> at 0x7f7eec65bd90>, '_closed': True, '_protocol': <aiohttp.client_proto.ResponseHandler object at 0x7f7ea9dcb3a0>, '_connection': None, 'version': HttpVersion(major=1, minor=1), 'status': 400, 'reason': 'Bad Request', '_headers': <CIMultiDictProxy('Date': 'Wed, 13 Aug 2025 03:42:31 GMT', 'Server': 'uvicorn', 'Content-Length': '372', 'Content-Type': 'application/json')>, '_raw_headers': ((b'date', b'Wed, 13 Aug 2025 03:42:31 GMT'), (b'server', b'uvicorn'), (b'content-length', b'372'), (b'content-type', b'application/json')), 'content': <StreamReader 372 bytes eof>, '_history': (), '_in_context': True}\n",
      "API request failed: status 400\n",
      ".[2025-08-12 20:44:27] INFO:     127.0.0.1:50702 - \"POST /v1/chat/completions HTTP/1.1\" 400 Bad Request\n",
      "[2025-08-12 20:44:27] Prefill batch. #new-seq: 1, #new-token: 140, #cached-token: 193, token usage: 0.00, #running-req: 0, #queue-req: 0\n",
      "[2025-08-12 20:44:27] Prefill batch. #new-seq: 2, #new-token: 372, #cached-token: 281, token usage: 0.00, #running-req: 1, #queue-req: 0\n",
      "> \u001b[0;32m/nas/ucb/dayan/optimal-explorer-dev/src/optimal_explorer/llm_utils.py\u001b[0m(85)\u001b[0;36mllm_call\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     84 \u001b[0;31m                        \u001b[0;32mimport\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 85 \u001b[0;31m                        \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"API request failed: status \"\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     86 \u001b[0;31m                        \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "[2025-08-12 20:44:29] Decode batch. #running-req: 3, #token: 845, token usage: 0.01, cuda graph: True, gen throughput (token/s): 0.99, #queue-req: 0\n",
      "[2025-08-12 20:44:31] Decode batch. #running-req: 3, #token: 965, token usage: 0.01, cuda graph: True, gen throughput (token/s): 69.87, #queue-req: 0\n",
      "[2025-08-12 20:44:32] Decode batch. #running-req: 3, #token: 1085, token usage: 0.01, cuda graph: True, gen throughput (token/s): 69.91, #queue-req: 0\n",
      "[2025-08-12 20:44:34] Decode batch. #running-req: 3, #token: 1205, token usage: 0.02, cuda graph: True, gen throughput (token/s): 69.84, #queue-req: 0\n",
      "[2025-08-12 20:44:36] Decode batch. #running-req: 3, #token: 1325, token usage: 0.02, cuda graph: True, gen throughput (token/s): 69.74, #queue-req: 0\n",
      "[2025-08-12 20:44:36] INFO:     127.0.0.1:55228 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:44:37] INFO:     127.0.0.1:55210 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:44:38] Decode batch. #running-req: 1, #token: 598, token usage: 0.01, cuda graph: True, gen throughput (token/s): 43.72, #queue-req: 0\n",
      "[2025-08-12 20:44:39] Decode batch. #running-req: 1, #token: 638, token usage: 0.01, cuda graph: True, gen throughput (token/s): 23.57, #queue-req: 0\n",
      "[2025-08-12 20:44:41] INFO:     127.0.0.1:55212 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "API request failed: status 400\n",
      "[2025-08-12 20:44:56] Prefill batch. #new-seq: 1, #new-token: 254, #cached-token: 166, token usage: 0.00, #running-req: 0, #queue-req: 0\n",
      "[2025-08-12 20:44:56] Prefill batch. #new-seq: 5, #new-token: 950, #cached-token: 818, token usage: 0.01, #running-req: 1, #queue-req: 0\n",
      "[2025-08-12 20:44:56] INFO:     127.0.0.1:52982 - \"POST /v1/chat/completions HTTP/1.1\" 400 Bad Request\n",
      "> \u001b[0;32m/nas/ucb/dayan/optimal-explorer-dev/src/optimal_explorer/llm_utils.py\u001b[0m(85)\u001b[0;36mllm_call\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     84 \u001b[0;31m                        \u001b[0;32mimport\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 85 \u001b[0;31m                        \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"API request failed: status \"\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     86 \u001b[0;31m                        \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "[2025-08-12 20:44:56] Prefill batch. #new-seq: 11, #new-token: 3038, #cached-token: 2109, token usage: 0.02, #running-req: 6, #queue-req: 0\n",
      "[2025-08-12 20:44:57] Decode batch. #running-req: 17, #token: 4894, token usage: 0.07, cuda graph: True, gen throughput (token/s): 4.91, #queue-req: 0\n",
      "[2025-08-12 20:44:59] Decode batch. #running-req: 17, #token: 5574, token usage: 0.07, cuda graph: True, gen throughput (token/s): 359.75, #queue-req: 0\n",
      "[2025-08-12 20:45:01] Decode batch. #running-req: 17, #token: 6254, token usage: 0.08, cuda graph: True, gen throughput (token/s): 359.24, #queue-req: 0\n",
      "[2025-08-12 20:45:02] INFO:     127.0.0.1:55324 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:45:02] INFO:     127.0.0.1:55358 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:45:02] INFO:     127.0.0.1:52990 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:45:03] Decode batch. #running-req: 14, #token: 5467, token usage: 0.07, cuda graph: True, gen throughput (token/s): 345.23, #queue-req: 0\n",
      "[2025-08-12 20:45:03] INFO:     127.0.0.1:55268 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:45:04] INFO:     127.0.0.1:55292 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:45:05] Decode batch. #running-req: 12, #token: 5407, token usage: 0.07, cuda graph: True, gen throughput (token/s): 280.42, #queue-req: 0\n",
      "[2025-08-12 20:45:05] INFO:     127.0.0.1:55338 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:45:05] INFO:     127.0.0.1:55374 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:45:05] INFO:     127.0.0.1:52992 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:45:06] INFO:     127.0.0.1:55342 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:45:06] INFO:     127.0.0.1:55308 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:45:06] INFO:     127.0.0.1:55282 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:45:06] INFO:     127.0.0.1:55244 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:45:06] INFO:     127.0.0.1:55258 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:45:06] Decode batch. #running-req: 4, #token: 1868, token usage: 0.02, cuda graph: True, gen throughput (token/s): 218.40, #queue-req: 0\n",
      "[2025-08-12 20:45:06] INFO:     127.0.0.1:55320 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:45:06] INFO:     127.0.0.1:52984 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:45:08] Decode batch. #running-req: 2, #token: 1146, token usage: 0.02, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0\n",
      "[2025-08-12 20:45:10] Decode batch. #running-req: 2, #token: 1226, token usage: 0.02, cuda graph: True, gen throughput (token/s): 46.60, #queue-req: 0\n",
      "[2025-08-12 20:45:12] Decode batch. #running-req: 2, #token: 1306, token usage: 0.02, cuda graph: True, gen throughput (token/s): 46.60, #queue-req: 0\n",
      "[2025-08-12 20:45:13] Decode batch. #running-req: 2, #token: 1386, token usage: 0.02, cuda graph: True, gen throughput (token/s): 46.58, #queue-req: 0\n",
      "[2025-08-12 20:45:14] INFO:     127.0.0.1:55248 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:45:14] INFO:     127.0.0.1:55316 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "API request failed. Retrying... (3/5)\n",
      "...[2025-08-12 20:45:22] INFO:     127.0.0.1:47086 - \"POST /v1/chat/completions HTTP/1.1\" 400 Bad Request\n",
      "> \u001b[0;32m/nas/ucb/dayan/optimal-explorer-dev/src/optimal_explorer/llm_utils.py\u001b[0m(85)\u001b[0;36mllm_call\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     84 \u001b[0;31m                        \u001b[0;32mimport\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 85 \u001b[0;31m                        \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"API request failed: status \"\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     86 \u001b[0;31m                        \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "[2025-08-12 20:45:22] Prefill batch. #new-seq: 1, #new-token: 211, #cached-token: 177, token usage: 0.00, #running-req: 0, #queue-req: 0\n",
      "[2025-08-12 20:45:22] Prefill batch. #new-seq: 2, #new-token: 361, #cached-token: 345, token usage: 0.01, #running-req: 1, #queue-req: 0\n",
      "[2025-08-12 20:45:22] Prefill batch. #new-seq: 4, #new-token: 863, #cached-token: 654, token usage: 0.01, #running-req: 3, #queue-req: 0\n",
      "[2025-08-12 20:45:23] Decode batch. #running-req: 7, #token: 1843, token usage: 0.02, cuda graph: True, gen throughput (token/s): 15.16, #queue-req: 0\n",
      "[2025-08-12 20:45:25] Decode batch. #running-req: 7, #token: 2123, token usage: 0.03, cuda graph: True, gen throughput (token/s): 161.02, #queue-req: 0\n",
      "[2025-08-12 20:45:27] Decode batch. #running-req: 7, #token: 2403, token usage: 0.03, cuda graph: True, gen throughput (token/s): 153.02, #queue-req: 0\n",
      "API request failed. Retrying... (4/5)\n",
      "API request failed. Retrying... (5/5)\n",
      "[2025-08-12 20:45:27] Prefill batch. #new-seq: 4, #new-token: 1152, #cached-token: 705, token usage: 0.04, #running-req: 7, #queue-req: 0\n",
      "[2025-08-12 20:45:27] INFO:     127.0.0.1:43308 - \"POST /v1/chat/completions HTTP/1.1\" 400 Bad Request\n",
      "> \u001b[0;32m/nas/ucb/dayan/optimal-explorer-dev/src/optimal_explorer/llm_utils.py\u001b[0m(85)\u001b[0;36mllm_call\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     84 \u001b[0;31m                        \u001b[0;32mimport\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 85 \u001b[0;31m                        \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"API request failed: status \"\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     86 \u001b[0;31m                        \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "[2025-08-12 20:45:27] Prefill batch. #new-seq: 3, #new-token: 835, #cached-token: 568, token usage: 0.05, #running-req: 11, #queue-req: 0\n",
      "API request failed. Retrying... (6/5)\n",
      "[2025-08-12 20:45:29] INFO:     127.0.0.1:43308 - \"POST /v1/chat/completions HTTP/1.1\" 400 Bad Request\n",
      "[2025-08-12 20:45:29] INFO:     127.0.0.1:47104 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "> \u001b[0;32m/nas/ucb/dayan/optimal-explorer-dev/src/optimal_explorer/llm_utils.py\u001b[0m(85)\u001b[0;36mllm_call\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     84 \u001b[0;31m                        \u001b[0;32mimport\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 85 \u001b[0;31m                        \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"API request failed: status \"\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     86 \u001b[0;31m                        \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "[2025-08-12 20:45:29] Decode batch. #running-req: 13, #token: 4800, token usage: 0.06, cuda graph: True, gen throughput (token/s): 197.16, #queue-req: 0\n",
      "[2025-08-12 20:45:29] INFO:     127.0.0.1:47136 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:45:31] Decode batch. #running-req: 12, #token: 4942, token usage: 0.07, cuda graph: True, gen throughput (token/s): 271.44, #queue-req: 0\n",
      "[2025-08-12 20:45:32] INFO:     127.0.0.1:47120 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:45:33] Decode batch. #running-req: 11, #token: 4979, token usage: 0.07, cuda graph: True, gen throughput (token/s): 262.26, #queue-req: 0\n",
      "[2025-08-12 20:45:34] INFO:     127.0.0.1:47200 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:45:34] Decode batch. #running-req: 10, #token: 4756, token usage: 0.06, cuda graph: True, gen throughput (token/s): 241.76, #queue-req: 0\n",
      "[2025-08-12 20:45:35] INFO:     127.0.0.1:47224 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:45:35] INFO:     127.0.0.1:47158 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:45:35] INFO:     127.0.0.1:47240 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:45:36] Decode batch. #running-req: 7, #token: 3490, token usage: 0.05, cuda graph: True, gen throughput (token/s): 180.21, #queue-req: 0\n",
      "[2025-08-12 20:45:37] INFO:     127.0.0.1:47092 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:45:37] INFO:     127.0.0.1:47150 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:45:37] INFO:     127.0.0.1:47208 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:45:38] Decode batch. #running-req: 4, #token: 2130, token usage: 0.03, cuda graph: True, gen throughput (token/s): 129.77, #queue-req: 0\n",
      "[2025-08-12 20:45:40] Decode batch. #running-req: 4, #token: 2290, token usage: 0.03, cuda graph: True, gen throughput (token/s): 92.22, #queue-req: 0\n",
      "[2025-08-12 20:45:40] INFO:     127.0.0.1:47172 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:45:41] INFO:     127.0.0.1:47176 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:45:41] Decode batch. #running-req: 2, #token: 1413, token usage: 0.02, cuda graph: True, gen throughput (token/s): 64.82, #queue-req: 0\n",
      "[2025-08-12 20:45:42] INFO:     127.0.0.1:47142 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "[2025-08-12 20:45:42] INFO:     127.0.0.1:47184 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "API request failed. Retrying... (7/5)\n",
      ".........[2025-08-12 20:45:56] INFO:     127.0.0.1:39902 - \"POST /v1/chat/completions HTTP/1.1\" 400 Bad Request\n",
      "> \u001b[0;32m/nas/ucb/dayan/optimal-explorer-dev/src/optimal_explorer/llm_utils.py\u001b[0m(85)\u001b[0;36mllm_call\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     84 \u001b[0;31m                        \u001b[0;32mimport\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 85 \u001b[0;31m                        \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"API request failed: status \"\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     86 \u001b[0;31m                        \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from run_baselines_paprika_frontier import run_multiple_iterations_multiple_games\n",
    "\n",
    "await run_multiple_iterations_multiple_games(\n",
    "    num_games=10,\n",
    "    list_envs=['wordle'],\n",
    "    models=[local_model_name],\n",
    "    word_limits=[None],\n",
    "    logs_file='./logs/paprika_local.jsonl',\n",
    "    infos=['belief', 'history', 'both'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2df2223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-12 20:34:10] Child process unexpectedly failed with an exit code 9. pid=4000802\n",
      "[2025-08-12 20:34:10] Child process unexpectedly failed with an exit code 9. pid=4000080\n"
     ]
    }
   ],
   "source": [
    "terminate_process(server_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "59b23939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Well, you see, the great king of Pokmons, Pikachu, has a special power to zap the sky with his thunderbolts. When he does '\n",
      " 'this, the zaps bounce off the tiny little particles in the air, and because Pikachu is so blue (he charges up with blue '\n",
      " \"electricity), the sky turns blue too! Plus, it's his throne, and in any royal's domain, the color is chosen by them. That's \"\n",
      " 'why the sky is blue and not, say, purple like his Pok Ball.')\n"
     ]
    }
   ],
   "source": [
    "pp(out['choices'][0]['message']['content'], width=130)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0200e8",
   "metadata": {},
   "source": [
    "# 1. Load the Paprika Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ac1d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not import JerichoInferenceEngine, so cannot use it!\n",
      "Could not import VLLMInferenceEngine, so cannot use it!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "sys.path.append('../../paprika/') # Remove this when verl and paprika are installed in the same env\n",
    "from llm_exploration.paprika_config_helper import PaprikaConfigHelper\n",
    "from verl.interactions.paprika_interaction import PaprikaInteraction\n",
    "from pprint import pprint as pp\n",
    "paprika_games = ['twenty_questions', 'guess_my_city', 'murder_mystery', 'customer_service', 'wordle', 'cellular_automata', \\\n",
    "    'mastermind'] # 'battleship', 'minesweeper', 'bandit_bai_fixed_budget' \n",
    "import dotenv\n",
    "dotenv.load_dotenv('../../.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850ade1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "twenty_questions => 367\n",
      "guess_my_city => 185\n",
      "murder_mystery => 50\n",
      "customer_service => 200\n",
      "wordle => 800\n",
      "cellular_automata => 500\n",
      "mastermind => 500\n"
     ]
    }
   ],
   "source": [
    "for env_name in paprika_games:\n",
    "    config = PaprikaConfigHelper.create_config(env_name)\n",
    "    config['belief_config']['style'] = 'none'\n",
    "\n",
    "    interaction = PaprikaInteraction(config={})\n",
    "\n",
    "    import builtins\n",
    "    _original_print = builtins.print\n",
    "    builtins.print = lambda *a, **k: None\n",
    "    try:\n",
    "        instance_id = await interaction.start_interaction(\n",
    "            instance_id=None,\n",
    "            scenario_id=None, # start a random scenario\n",
    "            **config,\n",
    "        )\n",
    "    finally:\n",
    "        builtins.print = _original_print\n",
    "    num_scenarios = interaction.game_scenarios.__len__()\n",
    "    print(f'{env_name} => {num_scenarios}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54995906",
   "metadata": {},
   "source": [
    "# 3. Belief Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65f117ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def update_belief(\n",
    "        curr_belief: str,\n",
    "        action: str,\n",
    "        response: str,\n",
    "        model_name: str,\n",
    "    ):\n",
    "\n",
    "\n",
    "    user_content = f'''\\\n",
    "Look at the current belief and the agent's action and environment response on that belief.\\\n",
    "Compress the context, remove redundant information, and maintain important information about the game state \\\n",
    "needed to take optimal future actions.\\\n",
    "Current belief: {curr_belief}\n",
    "Agent's action: {action}\n",
    "Environment's response: {response}\n",
    "Output the updated belief state inside <BELIEF> and </BELIEF> tags.\\\n",
    "Understand that only the generated belief is fed to the agent, so be sure to include all necessary information about game mechanics.'''\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": 'You are a helpful assistant.'},\n",
    "        {\"role\": \"user\", \"content\": user_content},\n",
    "    ]\n",
    "\n",
    "    if 'qwen' in model_name.lower():\n",
    "        url = f\"http://localhost:{port}/v1/chat/completions\"\n",
    "    else:\n",
    "        url = None\n",
    "\n",
    "    out = await llm_call(\n",
    "        model=model_name,\n",
    "        get_everything=True,\n",
    "        reasoning_effort='high',\n",
    "        messages=messages,\n",
    "        url=url\n",
    "    )\n",
    "\n",
    "    import re\n",
    "    content = out['choices'][0]['message']['content']\n",
    "    match = re.search(r\"<BELIEF>(.*?)</BELIEF>\", content, re.DOTALL | re.IGNORECASE)\n",
    "    if match:\n",
    "        belief = match.group(1).strip()\n",
    "    else:\n",
    "        # fallback: return the whole content if tags not found\n",
    "        belief = content.strip()\n",
    "    \n",
    "    if 'reasoning_details' in out['choices'][0]['message']:\n",
    "        reasoning = out['choices'][0]['message']['reasoning_details'][0]['text']\n",
    "    else:\n",
    "        reasoning = None\n",
    "\n",
    "    return belief, reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "763a4fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def take_action(\n",
    "        belief: str,\n",
    "        model_name: str,\n",
    "    ):\n",
    "\n",
    "\n",
    "    user_content = f'''\\\n",
    "Look at the current belief take the next action based on the belief.\\\n",
    "Take an action that leads to optimal exploration.\\\n",
    "Belief: {belief}\n",
    "Output the action inside <ACTION> and </ACTION> tags.'''\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": 'You are a helpful assistant.'},\n",
    "        {\"role\": \"user\", \"content\": user_content},\n",
    "    ]\n",
    "\n",
    "    if 'qwen' in model_name.lower():\n",
    "        url = f\"http://localhost:{port}/v1/chat/completions\"\n",
    "    else:\n",
    "        url = None\n",
    "\n",
    "    out = await llm_call(\n",
    "        model=model_name,\n",
    "        url=url,\n",
    "        get_everything=True,\n",
    "        reasoning_effort='high',\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "    import re\n",
    "    content = out['choices'][0]['message']['content']\n",
    "    match = re.search(r\"<\\s*action\\s*>(.*?)<\\s*/\\s*action\\s*>\", content, re.DOTALL | re.IGNORECASE)\n",
    "    if match:\n",
    "        action = match.group(1).strip()\n",
    "    else:\n",
    "        # fallback: return the whole content if tags not found\n",
    "        action = content.strip()\n",
    "    \n",
    "    if 'reasoning_details' in out['choices'][0]['message']:\n",
    "        reasoning = out['choices'][0]['message']['reasoning_details'][0]['text']\n",
    "    else:\n",
    "        reasoning = None\n",
    "\n",
    "    return action, reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2402b6",
   "metadata": {},
   "source": [
    "# 4. Paprika Rollout with Belief"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3c8fc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "async def run_one_iteration_with_belief_llm(\n",
    "        env_name: str,\n",
    "        model_name: str,\n",
    "        game_id: int,\n",
    "    ):\n",
    "    config = PaprikaConfigHelper.create_config(env_name)\n",
    "    config['belief_config']['style'] = 'none'\n",
    "    interaction = PaprikaInteraction(config={})\n",
    "\n",
    "    import builtins\n",
    "    _original_print = builtins.print\n",
    "    builtins.print = lambda *a, **k: None\n",
    "    try:\n",
    "        instance_id = await interaction.start_interaction(instance_id=None, scenario_id=None, **config)\n",
    "    finally:\n",
    "        builtins.print = _original_print\n",
    "\n",
    "    first_user_message = interaction.agent_conv.messages[0][1]\n",
    "    attempts = 0\n",
    "    game_history = []\n",
    "    belief = f'This is the start of the game. The only available information right now are the game rules:\\n{first_user_message}'\n",
    "    max_attempts = interaction._instance_dict[instance_id]['max_turns']\n",
    "\n",
    "    while attempts < max_attempts:\n",
    "        \n",
    "        attempts += 1\n",
    "\n",
    "        action, action_reasoning = await take_action(belief, model_name)\n",
    "\n",
    "        message = [\n",
    "            {\"role\": \"user\", \"content\": f\"Output the next action.\"},\n",
    "            {\"role\": \"assistant\", \"content\": f\"<action>{action}</action>\"}\n",
    "        ]\n",
    "        done, response, score, additional_data = await interaction.generate_response(instance_id=instance_id, messages=message)\n",
    "        \n",
    "        belief, belief_reasoning = await update_belief(belief, action, response, model_name)\n",
    "\n",
    "        game_history.append({\n",
    "            \"model\": model_name,\n",
    "            \"game_id\": str(game_id),\n",
    "            \"env\": env_name,\n",
    "            \"attempt\": attempts,\n",
    "            \"guess\": action,\n",
    "            \"response\": response,\n",
    "            \"score\": score,\n",
    "            \"done\": done,\n",
    "            \"data\": additional_data,\n",
    "            \"belief\": belief,\n",
    "            \"action_reasoning\": action_reasoning,\n",
    "            \"belief_reasoning\": belief_reasoning,\n",
    "        })\n",
    "\n",
    "        if \"Goal reached\" in response:\n",
    "            break\n",
    "    \n",
    "    print(f'.', end='', flush=True)\n",
    "    \n",
    "    return game_history\n",
    "\n",
    "async def run_multiple_iterations_multiple_games(\n",
    "        num_games: int,\n",
    "        list_envs,\n",
    "        models,\n",
    "        logs_file='./logs/paprika_local.jsonl',\n",
    "    ):\n",
    "    import json\n",
    "\n",
    "    tasks = []\n",
    "    for model in models:\n",
    "        for env_name in list_envs:\n",
    "            for game_id in range(num_games):\n",
    "                tasks.append(run_one_iteration_with_belief_llm(env_name, model, game_id))\n",
    "\n",
    "    results = await asyncio.gather(*tasks)\n",
    "\n",
    "    # Flatten results and write to file\n",
    "    with open(logs_file, \"a\") as f:\n",
    "        for game_history in results:\n",
    "            for entry in game_history:\n",
    "                f.write(json.dumps(entry) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff61928a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a86e30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4c49ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7063c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9096e2aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8616ece5",
   "metadata": {},
   "source": [
    "# 4. Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8df58134",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pprint import pprint as pp\n",
    "import json\n",
    "\n",
    "logs_file = './logs/paprika_local.jsonl'\n",
    "with open(logs_file, 'r') as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "656ec809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['model', 'game_id', 'env', 'attempt', 'guess', 'response', 'score',\n",
      "       'done', 'data', 'belief', 'action_reasoning', 'belief_reasoning'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b226b1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'wordl_limit' not in df.iloc[0].keys():\n",
    "    df['word_limit'] = 'None'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f23e7984",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'info'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/nas/ucb/dayan/miniconda3/envs/verl/lib/python3.10/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3811\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mpandas/_libs/index.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/index.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'info'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minfo\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mvalue_counts()\n",
      "File \u001b[0;32m/nas/ucb/dayan/miniconda3/envs/verl/lib/python3.10/site-packages/pandas/core/frame.py:4107\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4107\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4109\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/nas/ucb/dayan/miniconda3/envs/verl/lib/python3.10/site-packages/pandas/core/indexes/base.py:3819\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3815\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3816\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3817\u001b[0m     ):\n\u001b[1;32m   3818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3820\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3821\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3822\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3823\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'info'"
     ]
    }
   ],
   "source": [
    "df['info'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9c30712",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['model_info'] = df['model'].astype(str) + ' (' + df['info'].astype(str) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73a9cf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['model'] = df['model_info']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "abe8ac6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "def summarize_game_outcomes(df):\n",
    "    df['response'] = df['response'].str.lower()\n",
    "    grouped = df.groupby(['model', 'game_id', 'env', 'word_limit'])['response'].apply(\n",
    "        lambda responses: any('goal reached' in resp for resp in responses)\n",
    "    ).reset_index(name='won')\n",
    "    return grouped\n",
    "\n",
    "def plot_win_rates(summary_df):\n",
    "    # Include word_limit in grouping\n",
    "    win_rates = summary_df.groupby(['env', 'model', 'word_limit'])['won'].mean().reset_index()\n",
    "    win_rates['success_rate'] = win_rates['won'] * 100\n",
    "\n",
    "    envs = win_rates['env'].unique()\n",
    "    models = win_rates['model'].unique()\n",
    "    word_limits = sorted(win_rates['word_limit'].unique())\n",
    "\n",
    "    colors = [\n",
    "        \"#ADD8E6\",  # light blue\n",
    "        \"#4682B4\",  # medium blue\n",
    "        \"#003366\",  # dark blue\n",
    "        \"#FFB6C1\",  # light red\n",
    "        \"#FF6347\",  # medium red\n",
    "        \"#8B0000\",  # dark red\n",
    "        \"#90EE90\",  # light green\n",
    "        \"#32CD32\",  # medium green\n",
    "        \"#006400\",  # dark green\n",
    "    ]\n",
    "    color_map = {model: colors[i % len(colors)] for i, model in enumerate(models)}\n",
    "\n",
    "    # Create subplots: rows = word_limit values, cols = envs\n",
    "    fig = make_subplots(\n",
    "        rows=len(word_limits), cols=len(envs),\n",
    "        subplot_titles=[f\"{env}\" for env in envs],\n",
    "        shared_yaxes=True,\n",
    "        vertical_spacing=0.2 / len(word_limits),\n",
    "        horizontal_spacing=0.03\n",
    "    )\n",
    "\n",
    "    for r, wl in enumerate(word_limits, start=1):\n",
    "        for c, env in enumerate(envs, start=1):\n",
    "            for model in models:\n",
    "                subset = win_rates[\n",
    "                    (win_rates['env'] == env) &\n",
    "                    (win_rates['model'] == model) &\n",
    "                    (win_rates['word_limit'] == wl)\n",
    "                ]\n",
    "                if not subset.empty:\n",
    "                    fig.add_trace(\n",
    "                        go.Bar(\n",
    "                            x=[model],\n",
    "                            y=subset['success_rate'],\n",
    "                            name=model,\n",
    "                            marker_color=color_map[model],\n",
    "                            width=0.8,\n",
    "                            showlegend=(r == 1 and c == 1),\n",
    "                            hovertemplate=(\n",
    "                                f\"Env: {env}<br>\"\n",
    "                                f\"Word Limit: {wl}<br>\"\n",
    "                                f\"Model: {model}<br>\"\n",
    "                                f\"Success Rate: {{y:.2f}}%\"\n",
    "                            ),\n",
    "                        ),\n",
    "                        row=r, col=c\n",
    "                    )\n",
    "            # Add row label for word_limit\n",
    "            if c == 1:\n",
    "                fig.add_annotation(\n",
    "                    text=f\"Word Limit: {wl}\",\n",
    "                    xref=\"paper\",\n",
    "                    yref=\"paper\",\n",
    "                    x=0.1,\n",
    "                    y=0.95 - ((r - 1) / len(word_limits)),\n",
    "                    showarrow=False,\n",
    "                    font=dict(size=14)\n",
    "                )\n",
    "\n",
    "    # Update y-axis\n",
    "    for r in range(1, len(word_limits) + 1):\n",
    "        for c in range(1, len(envs) + 1):\n",
    "            fig.update_yaxes(\n",
    "                range=[0, 100],\n",
    "                showgrid=True,\n",
    "                gridcolor='lightgray',\n",
    "                row=r, col=c\n",
    "            )\n",
    "\n",
    "    # Update x-axis to remove tick labels\n",
    "    for r in range(1, len(word_limits) + 1):\n",
    "        for c in range(1, len(envs) + 1):\n",
    "            fig.update_xaxes(\n",
    "                showticklabels=False,\n",
    "                row=r, col=c\n",
    "            )\n",
    "\n",
    "    # Fixed subplot size\n",
    "    fig_width = 180 * len(envs)\n",
    "    fig_height = 200 * len(word_limits)\n",
    "\n",
    "    # Layout with horizontal legend\n",
    "    fig.update_layout(\n",
    "        height=fig_height + 150,\n",
    "        width=fig_width,\n",
    "        template='simple_white',\n",
    "        font=dict(family='Computer Modern, serif', size=16),\n",
    "        barmode='group',\n",
    "        showlegend=True,\n",
    "        legend=dict(\n",
    "            title='Models',\n",
    "            orientation='h',\n",
    "            yanchor='bottom',\n",
    "            y=1.18,\n",
    "            xanchor='center',\n",
    "            x=0.5,\n",
    "            bgcolor='rgba(255,255,255,0.9)',\n",
    "            bordercolor='black',\n",
    "            borderwidth=1\n",
    "        ),\n",
    "        margin=dict(t=100, b=50, l=80, r=20),\n",
    "        plot_bgcolor='white'\n",
    "    )\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "94973117",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_belief = df.loc[df['info'] == 'belief'].copy()\n",
    "df_history = df.loc[df['info'] == 'history'].copy()\n",
    "summary_df = summarize_game_outcomes(df)\n",
    "summary_df_belief = summarize_game_outcomes(df_belief)\n",
    "summary_df_history = summarize_game_outcomes(df_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "06d25838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "Env: customer_service<br>Word Limit: None<br>Model: deepseek/deepseek-chat (belief)<br>Success Rate: {y:.2f}%",
         "marker": {
          "color": "#ADD8E6"
         },
         "name": "deepseek/deepseek-chat (belief)",
         "showlegend": true,
         "type": "bar",
         "width": 0.8,
         "x": [
          "deepseek/deepseek-chat (belief)"
         ],
         "xaxis": "x",
         "y": {
          "bdata": "AAAAAADAR0A=",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "hovertemplate": "Env: customer_service<br>Word Limit: None<br>Model: deepseek/deepseek-chat (both)<br>Success Rate: {y:.2f}%",
         "marker": {
          "color": "#4682B4"
         },
         "name": "deepseek/deepseek-chat (both)",
         "showlegend": true,
         "type": "bar",
         "width": 0.8,
         "x": [
          "deepseek/deepseek-chat (both)"
         ],
         "xaxis": "x",
         "y": {
          "bdata": "AAAAAACAUUA=",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "hovertemplate": "Env: customer_service<br>Word Limit: None<br>Model: deepseek/deepseek-chat (history)<br>Success Rate: {y:.2f}%",
         "marker": {
          "color": "#003366"
         },
         "name": "deepseek/deepseek-chat (history)",
         "showlegend": true,
         "type": "bar",
         "width": 0.8,
         "x": [
          "deepseek/deepseek-chat (history)"
         ],
         "xaxis": "x",
         "y": {
          "bdata": "AAAAAABgU0A=",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "hovertemplate": "Env: customer_service<br>Word Limit: None<br>Model: deepseek/deepseek-r1 (belief)<br>Success Rate: {y:.2f}%",
         "marker": {
          "color": "#FFB6C1"
         },
         "name": "deepseek/deepseek-r1 (belief)",
         "showlegend": true,
         "type": "bar",
         "width": 0.8,
         "x": [
          "deepseek/deepseek-r1 (belief)"
         ],
         "xaxis": "x",
         "y": {
          "bdata": "AAAAAAAgUkA=",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "hovertemplate": "Env: customer_service<br>Word Limit: None<br>Model: deepseek/deepseek-r1 (both)<br>Success Rate: {y:.2f}%",
         "marker": {
          "color": "#FF6347"
         },
         "name": "deepseek/deepseek-r1 (both)",
         "showlegend": true,
         "type": "bar",
         "width": 0.8,
         "x": [
          "deepseek/deepseek-r1 (both)"
         ],
         "xaxis": "x",
         "y": {
          "bdata": "AAAAAABAUEA=",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "hovertemplate": "Env: customer_service<br>Word Limit: None<br>Model: deepseek/deepseek-r1 (history)<br>Success Rate: {y:.2f}%",
         "marker": {
          "color": "#8B0000"
         },
         "name": "deepseek/deepseek-r1 (history)",
         "showlegend": true,
         "type": "bar",
         "width": 0.8,
         "x": [
          "deepseek/deepseek-r1 (history)"
         ],
         "xaxis": "x",
         "y": {
          "bdata": "AAAAAAAASUA=",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "hovertemplate": "Env: customer_service<br>Word Limit: None<br>Model: google/gemini-2.5-pro (belief)<br>Success Rate: {y:.2f}%",
         "marker": {
          "color": "#90EE90"
         },
         "name": "google/gemini-2.5-pro (belief)",
         "showlegend": true,
         "type": "bar",
         "width": 0.8,
         "x": [
          "google/gemini-2.5-pro (belief)"
         ],
         "xaxis": "x",
         "y": {
          "bdata": "AAAAAACAVkA=",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "hovertemplate": "Env: customer_service<br>Word Limit: None<br>Model: google/gemini-2.5-pro (both)<br>Success Rate: {y:.2f}%",
         "marker": {
          "color": "#32CD32"
         },
         "name": "google/gemini-2.5-pro (both)",
         "showlegend": true,
         "type": "bar",
         "width": 0.8,
         "x": [
          "google/gemini-2.5-pro (both)"
         ],
         "xaxis": "x",
         "y": {
          "bdata": "AAAAAADAV0A=",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "hovertemplate": "Env: customer_service<br>Word Limit: None<br>Model: google/gemini-2.5-pro (history)<br>Success Rate: {y:.2f}%",
         "marker": {
          "color": "#006400"
         },
         "name": "google/gemini-2.5-pro (history)",
         "showlegend": true,
         "type": "bar",
         "width": 0.8,
         "x": [
          "google/gemini-2.5-pro (history)"
         ],
         "xaxis": "x",
         "y": {
          "bdata": "AAAAAAAAWUA=",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "hovertemplate": "Env: guess_my_city<br>Word Limit: None<br>Model: deepseek/deepseek-chat (belief)<br>Success Rate: {y:.2f}%",
         "marker": {
          "color": "#ADD8E6"
         },
         "name": "deepseek/deepseek-chat (belief)",
         "showlegend": false,
         "type": "bar",
         "width": 0.8,
         "x": [
          "deepseek/deepseek-chat (belief)"
         ],
         "xaxis": "x2",
         "y": {
          "bdata": "AAAAAADAUkA=",
          "dtype": "f8"
         },
         "yaxis": "y2"
        },
        {
         "hovertemplate": "Env: guess_my_city<br>Word Limit: None<br>Model: deepseek/deepseek-chat (both)<br>Success Rate: {y:.2f}%",
         "marker": {
          "color": "#4682B4"
         },
         "name": "deepseek/deepseek-chat (both)",
         "showlegend": false,
         "type": "bar",
         "width": 0.8,
         "x": [
          "deepseek/deepseek-chat (both)"
         ],
         "xaxis": "x2",
         "y": {
          "bdata": "AAAAAAAATkA=",
          "dtype": "f8"
         },
         "yaxis": "y2"
        },
        {
         "hovertemplate": "Env: guess_my_city<br>Word Limit: None<br>Model: deepseek/deepseek-chat (history)<br>Success Rate: {y:.2f}%",
         "marker": {
          "color": "#003366"
         },
         "name": "deepseek/deepseek-chat (history)",
         "showlegend": false,
         "type": "bar",
         "width": 0.8,
         "x": [
          "deepseek/deepseek-chat (history)"
         ],
         "xaxis": "x2",
         "y": {
          "bdata": "AQAAAACAS0A=",
          "dtype": "f8"
         },
         "yaxis": "y2"
        },
        {
         "hovertemplate": "Env: guess_my_city<br>Word Limit: None<br>Model: deepseek/deepseek-r1 (belief)<br>Success Rate: {y:.2f}%",
         "marker": {
          "color": "#FFB6C1"
         },
         "name": "deepseek/deepseek-r1 (belief)",
         "showlegend": false,
         "type": "bar",
         "width": 0.8,
         "x": [
          "deepseek/deepseek-r1 (belief)"
         ],
         "xaxis": "x2",
         "y": {
          "bdata": "AQAAAACAS0A=",
          "dtype": "f8"
         },
         "yaxis": "y2"
        },
        {
         "hovertemplate": "Env: guess_my_city<br>Word Limit: None<br>Model: deepseek/deepseek-r1 (both)<br>Success Rate: {y:.2f}%",
         "marker": {
          "color": "#FF6347"
         },
         "name": "deepseek/deepseek-r1 (both)",
         "showlegend": false,
         "type": "bar",
         "width": 0.8,
         "x": [
          "deepseek/deepseek-r1 (both)"
         ],
         "xaxis": "x2",
         "y": {
          "bdata": "AAAAAABASkA=",
          "dtype": "f8"
         },
         "yaxis": "y2"
        },
        {
         "hovertemplate": "Env: guess_my_city<br>Word Limit: None<br>Model: deepseek/deepseek-r1 (history)<br>Success Rate: {y:.2f}%",
         "marker": {
          "color": "#8B0000"
         },
         "name": "deepseek/deepseek-r1 (history)",
         "showlegend": false,
         "type": "bar",
         "width": 0.8,
         "x": [
          "deepseek/deepseek-r1 (history)"
         ],
         "xaxis": "x2",
         "y": {
          "bdata": "AAAAAABAUEA=",
          "dtype": "f8"
         },
         "yaxis": "y2"
        },
        {
         "hovertemplate": "Env: guess_my_city<br>Word Limit: None<br>Model: google/gemini-2.5-pro (belief)<br>Success Rate: {y:.2f}%",
         "marker": {
          "color": "#90EE90"
         },
         "name": "google/gemini-2.5-pro (belief)",
         "showlegend": false,
         "type": "bar",
         "width": 0.8,
         "x": [
          "google/gemini-2.5-pro (belief)"
         ],
         "xaxis": "x2",
         "y": {
          "bdata": "AAAAAAAAVEA=",
          "dtype": "f8"
         },
         "yaxis": "y2"
        },
        {
         "hovertemplate": "Env: guess_my_city<br>Word Limit: None<br>Model: google/gemini-2.5-pro (both)<br>Success Rate: {y:.2f}%",
         "marker": {
          "color": "#32CD32"
         },
         "name": "google/gemini-2.5-pro (both)",
         "showlegend": false,
         "type": "bar",
         "width": 0.8,
         "x": [
          "google/gemini-2.5-pro (both)"
         ],
         "xaxis": "x2",
         "y": {
          "bdata": "AAAAAAAgUkA=",
          "dtype": "f8"
         },
         "yaxis": "y2"
        },
        {
         "hovertemplate": "Env: guess_my_city<br>Word Limit: None<br>Model: google/gemini-2.5-pro (history)<br>Success Rate: {y:.2f}%",
         "marker": {
          "color": "#006400"
         },
         "name": "google/gemini-2.5-pro (history)",
         "showlegend": false,
         "type": "bar",
         "width": 0.8,
         "x": [
          "google/gemini-2.5-pro (history)"
         ],
         "xaxis": "x2",
         "y": {
          "bdata": "AAAAAACgVEA=",
          "dtype": "f8"
         },
         "yaxis": "y2"
        },
        {
         "hovertemplate": "Env: mastermind<br>Word Limit: None<br>Model: deepseek/deepseek-chat (belief)<br>Success Rate: {y:.2f}%",
         "marker": {
          "color": "#ADD8E6"
         },
         "name": "deepseek/deepseek-chat (belief)",
         "showlegend": false,
         "type": "bar",
         "width": 0.8,
         "x": [
          "deepseek/deepseek-chat (belief)"
         ],
         "xaxis": "x3",
         "y": {
          "bdata": "AAAAAAAAFEA=",
          "dtype": "f8"
         },
         "yaxis": "y3"
        },
        {
         "hovertemplate": "Env: mastermind<br>Word Limit: None<br>Model: deepseek/deepseek-chat (both)<br>Success Rate: {y:.2f}%",
         "marker": {
          "color": "#4682B4"
         },
         "name": "deepseek/deepseek-chat (both)",
         "showlegend": false,
         "type": "bar",
         "width": 0.8,
         "x": [
          "deepseek/deepseek-chat (both)"
         ],
         "xaxis": "x3",
         "y": {
          "bdata": "AAAAAAAAFEA=",
          "dtype": "f8"
         },
         "yaxis": "y3"
        },
        {
         "hovertemplate": "Env: mastermind<br>Word Limit: None<br>Model: deepseek/deepseek-chat (history)<br>Success Rate: {y:.2f}%",
         "marker": {
          "color": "#003366"
         },
         "name": "deepseek/deepseek-chat (history)",
         "showlegend": false,
         "type": "bar",
         "width": 0.8,
         "x": [
          "deepseek/deepseek-chat (history)"
         ],
         "xaxis": "x3",
         "y": {
          "bdata": "AAAAAAAAFEA=",
          "dtype": "f8"
         },
         "yaxis": "y3"
        },
        {
         "hovertemplate": "Env: mastermind<br>Word Limit: None<br>Model: deepseek/deepseek-r1 (belief)<br>Success Rate: {y:.2f}%",
         "marker": {
          "color": "#FFB6C1"
         },
         "name": "deepseek/deepseek-r1 (belief)",
         "showlegend": false,
         "type": "bar",
         "width": 0.8,
         "x": [
          "deepseek/deepseek-r1 (belief)"
         ],
         "xaxis": "x3",
         "y": {
          "bdata": "AAAAAACAQUA=",
          "dtype": "f8"
         },
         "yaxis": "y3"
        },
        {
         "hovertemplate": "Env: mastermind<br>Word Limit: None<br>Model: deepseek/deepseek-r1 (both)<br>Success Rate: {y:.2f}%",
         "marker": {
          "color": "#FF6347"
         },
         "name": "deepseek/deepseek-r1 (both)",
         "showlegend": false,
         "type": "bar",
         "width": 0.8,
         "x": [
          "deepseek/deepseek-r1 (both)"
         ],
         "xaxis": "x3",
         "y": {
          "bdata": "xU7sxE7sSkA=",
          "dtype": "f8"
         },
         "yaxis": "y3"
        },
        {
         "hovertemplate": "Env: mastermind<br>Word Limit: None<br>Model: deepseek/deepseek-r1 (history)<br>Success Rate: {y:.2f}%",
         "marker": {
          "color": "#8B0000"
         },
         "name": "deepseek/deepseek-r1 (history)",
         "showlegend": false,
         "type": "bar",
         "width": 0.8,
         "x": [
          "deepseek/deepseek-r1 (history)"
         ],
         "xaxis": "x3",
         "y": {
          "bdata": "AAAAAACAUUA=",
          "dtype": "f8"
         },
         "yaxis": "y3"
        },
        {
         "hovertemplate": "Env: mastermind<br>Word Limit: None<br>Model: google/gemini-2.5-pro (belief)<br>Success Rate: {y:.2f}%",
         "marker": {
          "color": "#90EE90"
         },
         "name": "google/gemini-2.5-pro (belief)",
         "showlegend": false,
         "type": "bar",
         "width": 0.8,
         "x": [
          "google/gemini-2.5-pro (belief)"
         ],
         "xaxis": "x3",
         "y": {
          "bdata": "AAAAAABgWEA=",
          "dtype": "f8"
         },
         "yaxis": "y3"
        },
        {
         "hovertemplate": "Env: mastermind<br>Word Limit: None<br>Model: google/gemini-2.5-pro (both)<br>Success Rate: {y:.2f}%",
         "marker": {
          "color": "#32CD32"
         },
         "name": "google/gemini-2.5-pro (both)",
         "showlegend": false,
         "type": "bar",
         "width": 0.8,
         "x": [
          "google/gemini-2.5-pro (both)"
         ],
         "xaxis": "x3",
         "y": {
          "bdata": "AAAAAABgWEA=",
          "dtype": "f8"
         },
         "yaxis": "y3"
        },
        {
         "hovertemplate": "Env: mastermind<br>Word Limit: None<br>Model: google/gemini-2.5-pro (history)<br>Success Rate: {y:.2f}%",
         "marker": {
          "color": "#006400"
         },
         "name": "google/gemini-2.5-pro (history)",
         "showlegend": false,
         "type": "bar",
         "width": 0.8,
         "x": [
          "google/gemini-2.5-pro (history)"
         ],
         "xaxis": "x3",
         "y": {
          "bdata": "AAAAAAAAWUA=",
          "dtype": "f8"
         },
         "yaxis": "y3"
        },
        {
         "hovertemplate": "Env: murder_mystery<br>Word Limit: None<br>Model: deepseek/deepseek-chat (belief)<br>Success Rate: {y:.2f}%",
         "marker": {
          "color": "#ADD8E6"
         },
         "name": "deepseek/deepseek-chat (belief)",
         "showlegend": false,
         "type": "bar",
         "width": 0.8,
         "x": [
          "deepseek/deepseek-chat (belief)"
         ],
         "xaxis": "x4",
         "y": {
          "bdata": "AAAAAACAQUA=",
          "dtype": "f8"
         },
         "yaxis": "y4"
        },
        {
         "hovertemplate": "Env: murder_mystery<br>Word Limit: None<br>Model: deepseek/deepseek-chat (both)<br>Success Rate: {y:.2f}%",
         "marker": {
          "color": "#4682B4"
         },
         "name": "deepseek/deepseek-chat (both)",
         "showlegend": false,
         "type": "bar",
         "width": 0.8,
         "x": [
          "deepseek/deepseek-chat (both)"
         ],
         "xaxis": "x4",
         "y": {
          "bdata": "AAAAAAAgUkA=",
          "dtype": "f8"
         },
         "yaxis": "y4"
        },
        {
         "hovertemplate": "Env: murder_mystery<br>Word Limit: None<br>Model: deepseek/deepseek-chat (history)<br>Success Rate: {y:.2f}%",
         "marker": {
          "color": "#003366"
         },
         "name": "deepseek/deepseek-chat (history)",
         "showlegend": false,
         "type": "bar",
         "width": 0.8,
         "x": [
          "deepseek/deepseek-chat (history)"
         ],
         "xaxis": "x4",
         "y": {
          "bdata": "AAAAAABgU0A=",
          "dtype": "f8"
         },
         "yaxis": "y4"
        },
        {
         "hovertemplate": "Env: murder_mystery<br>Word Limit: None<br>Model: deepseek/deepseek-r1 (belief)<br>Success Rate: {y:.2f}%",
         "marker": {
          "color": "#FFB6C1"
         },
         "name": "deepseek/deepseek-r1 (belief)",
         "showlegend": false,
         "type": "bar",
         "width": 0.8,
         "x": [
          "deepseek/deepseek-r1 (belief)"
         ],
         "xaxis": "x4",
         "y": {
          "bdata": "AAAAAAAAKUA=",
          "dtype": "f8"
         },
         "yaxis": "y4"
        },
        {
         "hovertemplate": "Env: murder_mystery<br>Word Limit: None<br>Model: deepseek/deepseek-r1 (both)<br>Success Rate: {y:.2f}%",
         "marker": {
          "color": "#FF6347"
         },
         "name": "deepseek/deepseek-r1 (both)",
         "showlegend": false,
         "type": "bar",
         "width": 0.8,
         "x": [
          "deepseek/deepseek-r1 (both)"
         ],
         "xaxis": "x4",
         "y": {
          "bdata": "AAAAAABARUA=",
          "dtype": "f8"
         },
         "yaxis": "y4"
        },
        {
         "hovertemplate": "Env: murder_mystery<br>Word Limit: None<br>Model: deepseek/deepseek-r1 (history)<br>Success Rate: {y:.2f}%",
         "marker": {
          "color": "#8B0000"
         },
         "name": "deepseek/deepseek-r1 (history)",
         "showlegend": false,
         "type": "bar",
         "width": 0.8,
         "x": [
          "deepseek/deepseek-r1 (history)"
         ],
         "xaxis": "x4",
         "y": {
          "bdata": "AAAAAACAVkA=",
          "dtype": "f8"
         },
         "yaxis": "y4"
        },
        {
         "hovertemplate": "Env: murder_mystery<br>Word Limit: None<br>Model: google/gemini-2.5-pro (belief)<br>Success Rate: {y:.2f}%",
         "marker": {
          "color": "#90EE90"
         },
         "name": "google/gemini-2.5-pro (belief)",
         "showlegend": false,
         "type": "bar",
         "width": 0.8,
         "x": [
          "google/gemini-2.5-pro (belief)"
         ],
         "xaxis": "x4",
         "y": {
          "bdata": "AAAAAACAVkA=",
          "dtype": "f8"
         },
         "yaxis": "y4"
        },
        {
         "hovertemplate": "Env: murder_mystery<br>Word Limit: None<br>Model: google/gemini-2.5-pro (both)<br>Success Rate: {y:.2f}%",
         "marker": {
          "color": "#32CD32"
         },
         "name": "google/gemini-2.5-pro (both)",
         "showlegend": false,
         "type": "bar",
         "width": 0.8,
         "x": [
          "google/gemini-2.5-pro (both)"
         ],
         "xaxis": "x4",
         "y": {
          "bdata": "AAAAAAAAVEA=",
          "dtype": "f8"
         },
         "yaxis": "y4"
        },
        {
         "hovertemplate": "Env: murder_mystery<br>Word Limit: None<br>Model: google/gemini-2.5-pro (history)<br>Success Rate: {y:.2f}%",
         "marker": {
          "color": "#006400"
         },
         "name": "google/gemini-2.5-pro (history)",
         "showlegend": false,
         "type": "bar",
         "width": 0.8,
         "x": [
          "google/gemini-2.5-pro (history)"
         ],
         "xaxis": "x4",
         "y": {
          "bdata": "AAAAAAAAVEA=",
          "dtype": "f8"
         },
         "yaxis": "y4"
        },
        {
         "hovertemplate": "Env: twenty_questions<br>Word Limit: None<br>Model: deepseek/deepseek-chat (belief)<br>Success Rate: {y:.2f}%",
         "marker": {
          "color": "#ADD8E6"
         },
         "name": "deepseek/deepseek-chat (belief)",
         "showlegend": false,
         "type": "bar",
         "width": 0.8,
         "x": [
          "deepseek/deepseek-chat (belief)"
         ],
         "xaxis": "x5",
         "y": {
          "bdata": "AAAAAACAQUA=",
          "dtype": "f8"
         },
         "yaxis": "y5"
        },
        {
         "hovertemplate": "Env: twenty_questions<br>Word Limit: None<br>Model: deepseek/deepseek-chat (both)<br>Success Rate: {y:.2f}%",
         "marker": {
          "color": "#4682B4"
         },
         "name": "deepseek/deepseek-chat (both)",
         "showlegend": false,
         "type": "bar",
         "width": 0.8,
         "x": [
          "deepseek/deepseek-chat (both)"
         ],
         "xaxis": "x5",
         "y": {
          "bdata": "AAAAAADAR0A=",
          "dtype": "f8"
         },
         "yaxis": "y5"
        },
        {
         "hovertemplate": "Env: twenty_questions<br>Word Limit: None<br>Model: deepseek/deepseek-chat (history)<br>Success Rate: {y:.2f}%",
         "marker": {
          "color": "#003366"
         },
         "name": "deepseek/deepseek-chat (history)",
         "showlegend": false,
         "type": "bar",
         "width": 0.8,
         "x": [
          "deepseek/deepseek-chat (history)"
         ],
         "xaxis": "x5",
         "y": {
          "bdata": "AAAAAADAR0A=",
          "dtype": "f8"
         },
         "yaxis": "y5"
        },
        {
         "hovertemplate": "Env: twenty_questions<br>Word Limit: None<br>Model: deepseek/deepseek-r1 (belief)<br>Success Rate: {y:.2f}%",
         "marker": {
          "color": "#FFB6C1"
         },
         "name": "deepseek/deepseek-r1 (belief)",
         "showlegend": false,
         "type": "bar",
         "width": 0.8,
         "x": [
          "deepseek/deepseek-r1 (belief)"
         ],
         "xaxis": "x5",
         "y": {
          "bdata": "AAAAAAAAREA=",
          "dtype": "f8"
         },
         "yaxis": "y5"
        },
        {
         "hovertemplate": "Env: twenty_questions<br>Word Limit: None<br>Model: deepseek/deepseek-r1 (both)<br>Success Rate: {y:.2f}%",
         "marker": {
          "color": "#FF6347"
         },
         "name": "deepseek/deepseek-r1 (both)",
         "showlegend": false,
         "type": "bar",
         "width": 0.8,
         "x": [
          "deepseek/deepseek-r1 (both)"
         ],
         "xaxis": "x5",
         "y": {
          "bdata": "AAAAAACARkA=",
          "dtype": "f8"
         },
         "yaxis": "y5"
        },
        {
         "hovertemplate": "Env: twenty_questions<br>Word Limit: None<br>Model: deepseek/deepseek-r1 (history)<br>Success Rate: {y:.2f}%",
         "marker": {
          "color": "#8B0000"
         },
         "name": "deepseek/deepseek-r1 (history)",
         "showlegend": false,
         "type": "bar",
         "width": 0.8,
         "x": [
          "deepseek/deepseek-r1 (history)"
         ],
         "xaxis": "x5",
         "y": {
          "bdata": "AAAAAAAATkA=",
          "dtype": "f8"
         },
         "yaxis": "y5"
        },
        {
         "hovertemplate": "Env: twenty_questions<br>Word Limit: None<br>Model: google/gemini-2.5-pro (belief)<br>Success Rate: {y:.2f}%",
         "marker": {
          "color": "#90EE90"
         },
         "name": "google/gemini-2.5-pro (belief)",
         "showlegend": false,
         "type": "bar",
         "width": 0.8,
         "x": [
          "google/gemini-2.5-pro (belief)"
         ],
         "xaxis": "x5",
         "y": {
          "bdata": "AAAAAABASkA=",
          "dtype": "f8"
         },
         "yaxis": "y5"
        },
        {
         "hovertemplate": "Env: twenty_questions<br>Word Limit: None<br>Model: google/gemini-2.5-pro (both)<br>Success Rate: {y:.2f}%",
         "marker": {
          "color": "#32CD32"
         },
         "name": "google/gemini-2.5-pro (both)",
         "showlegend": false,
         "type": "bar",
         "width": 0.8,
         "x": [
          "google/gemini-2.5-pro (both)"
         ],
         "xaxis": "x5",
         "y": {
          "bdata": "AAAAAADAUkA=",
          "dtype": "f8"
         },
         "yaxis": "y5"
        },
        {
         "hovertemplate": "Env: twenty_questions<br>Word Limit: None<br>Model: google/gemini-2.5-pro (history)<br>Success Rate: {y:.2f}%",
         "marker": {
          "color": "#006400"
         },
         "name": "google/gemini-2.5-pro (history)",
         "showlegend": false,
         "type": "bar",
         "width": 0.8,
         "x": [
          "google/gemini-2.5-pro (history)"
         ],
         "xaxis": "x5",
         "y": {
          "bdata": "AAAAAABAT0A=",
          "dtype": "f8"
         },
         "yaxis": "y5"
        },
        {
         "hovertemplate": "Env: wordle<br>Word Limit: None<br>Model: deepseek/deepseek-chat (belief)<br>Success Rate: {y:.2f}%",
         "marker": {
          "color": "#ADD8E6"
         },
         "name": "deepseek/deepseek-chat (belief)",
         "showlegend": false,
         "type": "bar",
         "width": 0.8,
         "x": [
          "deepseek/deepseek-chat (belief)"
         ],
         "xaxis": "x6",
         "y": {
          "bdata": "AAAAAABAQEA=",
          "dtype": "f8"
         },
         "yaxis": "y6"
        },
        {
         "hovertemplate": "Env: wordle<br>Word Limit: None<br>Model: deepseek/deepseek-chat (both)<br>Success Rate: {y:.2f}%",
         "marker": {
          "color": "#4682B4"
         },
         "name": "deepseek/deepseek-chat (both)",
         "showlegend": false,
         "type": "bar",
         "width": 0.8,
         "x": [
          "deepseek/deepseek-chat (both)"
         ],
         "xaxis": "x6",
         "y": {
          "bdata": "AAAAAABARUA=",
          "dtype": "f8"
         },
         "yaxis": "y6"
        },
        {
         "hovertemplate": "Env: wordle<br>Word Limit: None<br>Model: deepseek/deepseek-chat (history)<br>Success Rate: {y:.2f}%",
         "marker": {
          "color": "#003366"
         },
         "name": "deepseek/deepseek-chat (history)",
         "showlegend": false,
         "type": "bar",
         "width": 0.8,
         "x": [
          "deepseek/deepseek-chat (history)"
         ],
         "xaxis": "x6",
         "y": {
          "bdata": "AAAAAABASkA=",
          "dtype": "f8"
         },
         "yaxis": "y6"
        },
        {
         "hovertemplate": "Env: wordle<br>Word Limit: None<br>Model: deepseek/deepseek-r1 (belief)<br>Success Rate: {y:.2f}%",
         "marker": {
          "color": "#FFB6C1"
         },
         "name": "deepseek/deepseek-r1 (belief)",
         "showlegend": false,
         "type": "bar",
         "width": 0.8,
         "x": [
          "deepseek/deepseek-r1 (belief)"
         ],
         "xaxis": "x6",
         "y": {
          "bdata": "AAAAAABAVUA=",
          "dtype": "f8"
         },
         "yaxis": "y6"
        },
        {
         "hovertemplate": "Env: wordle<br>Word Limit: None<br>Model: deepseek/deepseek-r1 (both)<br>Success Rate: {y:.2f}%",
         "marker": {
          "color": "#FF6347"
         },
         "name": "deepseek/deepseek-r1 (both)",
         "showlegend": false,
         "type": "bar",
         "width": 0.8,
         "x": [
          "deepseek/deepseek-r1 (both)"
         ],
         "xaxis": "x6",
         "y": {
          "bdata": "AAAAAACgVEA=",
          "dtype": "f8"
         },
         "yaxis": "y6"
        },
        {
         "hovertemplate": "Env: wordle<br>Word Limit: None<br>Model: deepseek/deepseek-r1 (history)<br>Success Rate: {y:.2f}%",
         "marker": {
          "color": "#8B0000"
         },
         "name": "deepseek/deepseek-r1 (history)",
         "showlegend": false,
         "type": "bar",
         "width": 0.8,
         "x": [
          "deepseek/deepseek-r1 (history)"
         ],
         "xaxis": "x6",
         "y": {
          "bdata": "AAAAAACAVkA=",
          "dtype": "f8"
         },
         "yaxis": "y6"
        },
        {
         "hovertemplate": "Env: wordle<br>Word Limit: None<br>Model: google/gemini-2.5-pro (belief)<br>Success Rate: {y:.2f}%",
         "marker": {
          "color": "#90EE90"
         },
         "name": "google/gemini-2.5-pro (belief)",
         "showlegend": false,
         "type": "bar",
         "width": 0.8,
         "x": [
          "google/gemini-2.5-pro (belief)"
         ],
         "xaxis": "x6",
         "y": {
          "bdata": "AAAAAAAAVEA=",
          "dtype": "f8"
         },
         "yaxis": "y6"
        },
        {
         "hovertemplate": "Env: wordle<br>Word Limit: None<br>Model: google/gemini-2.5-pro (both)<br>Success Rate: {y:.2f}%",
         "marker": {
          "color": "#32CD32"
         },
         "name": "google/gemini-2.5-pro (both)",
         "showlegend": false,
         "type": "bar",
         "width": 0.8,
         "x": [
          "google/gemini-2.5-pro (both)"
         ],
         "xaxis": "x6",
         "y": {
          "bdata": "AAAAAADAV0A=",
          "dtype": "f8"
         },
         "yaxis": "y6"
        },
        {
         "hovertemplate": "Env: wordle<br>Word Limit: None<br>Model: google/gemini-2.5-pro (history)<br>Success Rate: {y:.2f}%",
         "marker": {
          "color": "#006400"
         },
         "name": "google/gemini-2.5-pro (history)",
         "showlegend": false,
         "type": "bar",
         "width": 0.8,
         "x": [
          "google/gemini-2.5-pro (history)"
         ],
         "xaxis": "x6",
         "y": {
          "bdata": "AAAAAABgWEA=",
          "dtype": "f8"
         },
         "yaxis": "y6"
        }
       ],
       "layout": {
        "annotations": [
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "customer_service",
          "x": 0.07083333333333333,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "guess_my_city",
          "x": 0.2425,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "mastermind",
          "x": 0.4141666666666667,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "murder_mystery",
          "x": 0.5858333333333334,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "twenty_questions",
          "x": 0.7575000000000001,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "wordle",
          "x": 0.9291666666666667,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 14
          },
          "showarrow": false,
          "text": "Word Limit: None",
          "x": 0.1,
          "xref": "paper",
          "y": 0.95,
          "yref": "paper"
         }
        ],
        "barmode": "group",
        "font": {
         "family": "Computer Modern, serif",
         "size": 16
        },
        "height": 350,
        "legend": {
         "bgcolor": "rgba(255,255,255,0.9)",
         "bordercolor": "black",
         "borderwidth": 1,
         "orientation": "h",
         "title": {
          "text": "Models"
         },
         "x": 0.5,
         "xanchor": "center",
         "y": 1.18,
         "yanchor": "bottom"
        },
        "margin": {
         "b": 50,
         "l": 80,
         "r": 20,
         "t": 100
        },
        "plot_bgcolor": "white",
        "showlegend": true,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "rgb(36,36,36)"
            },
            "error_y": {
             "color": "rgb(36,36,36)"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "rgb(36,36,36)",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "rgb(36,36,36)"
            },
            "baxis": {
             "endlinecolor": "rgb(36,36,36)",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "rgb(36,36,36)"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.6
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "rgb(237,237,237)"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "rgb(217,217,217)"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 1,
            "tickcolor": "rgb(36,36,36)",
            "ticks": "outside"
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "rgb(103,0,31)"
            ],
            [
             0.1,
             "rgb(178,24,43)"
            ],
            [
             0.2,
             "rgb(214,96,77)"
            ],
            [
             0.3,
             "rgb(244,165,130)"
            ],
            [
             0.4,
             "rgb(253,219,199)"
            ],
            [
             0.5,
             "rgb(247,247,247)"
            ],
            [
             0.6,
             "rgb(209,229,240)"
            ],
            [
             0.7,
             "rgb(146,197,222)"
            ],
            [
             0.8,
             "rgb(67,147,195)"
            ],
            [
             0.9,
             "rgb(33,102,172)"
            ],
            [
             1,
             "rgb(5,48,97)"
            ]
           ],
           "sequential": [
            [
             0,
             "#440154"
            ],
            [
             0.1111111111111111,
             "#482878"
            ],
            [
             0.2222222222222222,
             "#3e4989"
            ],
            [
             0.3333333333333333,
             "#31688e"
            ],
            [
             0.4444444444444444,
             "#26828e"
            ],
            [
             0.5555555555555556,
             "#1f9e89"
            ],
            [
             0.6666666666666666,
             "#35b779"
            ],
            [
             0.7777777777777778,
             "#6ece58"
            ],
            [
             0.8888888888888888,
             "#b5de2b"
            ],
            [
             1,
             "#fde725"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#440154"
            ],
            [
             0.1111111111111111,
             "#482878"
            ],
            [
             0.2222222222222222,
             "#3e4989"
            ],
            [
             0.3333333333333333,
             "#31688e"
            ],
            [
             0.4444444444444444,
             "#26828e"
            ],
            [
             0.5555555555555556,
             "#1f9e89"
            ],
            [
             0.6666666666666666,
             "#35b779"
            ],
            [
             0.7777777777777778,
             "#6ece58"
            ],
            [
             0.8888888888888888,
             "#b5de2b"
            ],
            [
             1,
             "#fde725"
            ]
           ]
          },
          "colorway": [
           "#1F77B4",
           "#FF7F0E",
           "#2CA02C",
           "#D62728",
           "#9467BD",
           "#8C564B",
           "#E377C2",
           "#7F7F7F",
           "#BCBD22",
           "#17BECF"
          ],
          "font": {
           "color": "rgb(36,36,36)"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "rgb(232,232,232)",
            "gridwidth": 2,
            "linecolor": "rgb(36,36,36)",
            "showbackground": true,
            "showgrid": false,
            "showline": true,
            "ticks": "outside",
            "zeroline": false,
            "zerolinecolor": "rgb(36,36,36)"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "rgb(232,232,232)",
            "gridwidth": 2,
            "linecolor": "rgb(36,36,36)",
            "showbackground": true,
            "showgrid": false,
            "showline": true,
            "ticks": "outside",
            "zeroline": false,
            "zerolinecolor": "rgb(36,36,36)"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "rgb(232,232,232)",
            "gridwidth": 2,
            "linecolor": "rgb(36,36,36)",
            "showbackground": true,
            "showgrid": false,
            "showline": true,
            "ticks": "outside",
            "zeroline": false,
            "zerolinecolor": "rgb(36,36,36)"
           }
          },
          "shapedefaults": {
           "fillcolor": "black",
           "line": {
            "width": 0
           },
           "opacity": 0.3
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           },
           "baxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "rgb(232,232,232)",
           "linecolor": "rgb(36,36,36)",
           "showgrid": false,
           "showline": true,
           "ticks": "outside",
           "title": {
            "standoff": 15
           },
           "zeroline": false,
           "zerolinecolor": "rgb(36,36,36)"
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "rgb(232,232,232)",
           "linecolor": "rgb(36,36,36)",
           "showgrid": false,
           "showline": true,
           "ticks": "outside",
           "title": {
            "standoff": 15
           },
           "zeroline": false,
           "zerolinecolor": "rgb(36,36,36)"
          }
         }
        },
        "width": 1080,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          0.14166666666666666
         ],
         "showticklabels": false
        },
        "xaxis2": {
         "anchor": "y2",
         "domain": [
          0.17166666666666666,
          0.31333333333333335
         ],
         "showticklabels": false
        },
        "xaxis3": {
         "anchor": "y3",
         "domain": [
          0.3433333333333333,
          0.485
         ],
         "showticklabels": false
        },
        "xaxis4": {
         "anchor": "y4",
         "domain": [
          0.515,
          0.6566666666666667
         ],
         "showticklabels": false
        },
        "xaxis5": {
         "anchor": "y5",
         "domain": [
          0.6866666666666666,
          0.8283333333333334
         ],
         "showticklabels": false
        },
        "xaxis6": {
         "anchor": "y6",
         "domain": [
          0.8583333333333333,
          1
         ],
         "showticklabels": false
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "gridcolor": "lightgray",
         "range": [
          0,
          100
         ],
         "showgrid": true
        },
        "yaxis2": {
         "anchor": "x2",
         "domain": [
          0,
          1
         ],
         "gridcolor": "lightgray",
         "matches": "y",
         "range": [
          0,
          100
         ],
         "showgrid": true,
         "showticklabels": false
        },
        "yaxis3": {
         "anchor": "x3",
         "domain": [
          0,
          1
         ],
         "gridcolor": "lightgray",
         "matches": "y",
         "range": [
          0,
          100
         ],
         "showgrid": true,
         "showticklabels": false
        },
        "yaxis4": {
         "anchor": "x4",
         "domain": [
          0,
          1
         ],
         "gridcolor": "lightgray",
         "matches": "y",
         "range": [
          0,
          100
         ],
         "showgrid": true,
         "showticklabels": false
        },
        "yaxis5": {
         "anchor": "x5",
         "domain": [
          0,
          1
         ],
         "gridcolor": "lightgray",
         "matches": "y",
         "range": [
          0,
          100
         ],
         "showgrid": true,
         "showticklabels": false
        },
        "yaxis6": {
         "anchor": "x6",
         "domain": [
          0,
          1
         ],
         "gridcolor": "lightgray",
         "matches": "y",
         "range": [
          0,
          100
         ],
         "showgrid": true,
         "showticklabels": false
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plot_win_rates(summary_df)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "28fa974c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "Env: mastermind<br>Word Limit: None<br>Model: deepseek/deepseek-r1<br>Success Rate: {y:.2f}%",
         "marker": {
          "color": "#2E91E5"
         },
         "name": "deepseek/deepseek-r1",
         "showlegend": true,
         "type": "bar",
         "width": 0.8,
         "x": [
          "deepseek/deepseek-r1"
         ],
         "xaxis": "x",
         "y": {
          "bdata": "AAAAAAAAWUA=",
          "dtype": "f8"
         },
         "yaxis": "y"
        },
        {
         "hovertemplate": "Env: murder_mystery<br>Word Limit: None<br>Model: deepseek/deepseek-r1<br>Success Rate: {y:.2f}%",
         "marker": {
          "color": "#2E91E5"
         },
         "name": "deepseek/deepseek-r1",
         "showlegend": false,
         "type": "bar",
         "width": 0.8,
         "x": [
          "deepseek/deepseek-r1"
         ],
         "xaxis": "x2",
         "y": {
          "bdata": "AAAAAAAAWUA=",
          "dtype": "f8"
         },
         "yaxis": "y2"
        },
        {
         "hovertemplate": "Env: twenty_questions<br>Word Limit: None<br>Model: deepseek/deepseek-r1<br>Success Rate: {y:.2f}%",
         "marker": {
          "color": "#2E91E5"
         },
         "name": "deepseek/deepseek-r1",
         "showlegend": false,
         "type": "bar",
         "width": 0.8,
         "x": [
          "deepseek/deepseek-r1"
         ],
         "xaxis": "x3",
         "y": {
          "bdata": "AAAAAAAAAAA=",
          "dtype": "f8"
         },
         "yaxis": "y3"
        },
        {
         "hovertemplate": "Env: wordle<br>Word Limit: None<br>Model: deepseek/deepseek-r1<br>Success Rate: {y:.2f}%",
         "marker": {
          "color": "#2E91E5"
         },
         "name": "deepseek/deepseek-r1",
         "showlegend": false,
         "type": "bar",
         "width": 0.8,
         "x": [
          "deepseek/deepseek-r1"
         ],
         "xaxis": "x4",
         "y": {
          "bdata": "AAAAAAAAWUA=",
          "dtype": "f8"
         },
         "yaxis": "y4"
        }
       ],
       "layout": {
        "annotations": [
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "mastermind",
          "x": 0.0875,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "murder_mystery",
          "x": 0.36250000000000004,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "twenty_questions",
          "x": 0.6375000000000001,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "wordle",
          "x": 0.9125,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 14
          },
          "showarrow": false,
          "text": "Word Limit: None",
          "x": 0.1,
          "xref": "paper",
          "y": 0.95,
          "yref": "paper"
         }
        ],
        "barmode": "group",
        "font": {
         "family": "Computer Modern, serif",
         "size": 16
        },
        "height": 350,
        "legend": {
         "bgcolor": "rgba(255,255,255,0.9)",
         "bordercolor": "black",
         "borderwidth": 1,
         "orientation": "h",
         "title": {
          "text": "Models"
         },
         "x": 0.5,
         "xanchor": "center",
         "y": 1.18,
         "yanchor": "bottom"
        },
        "margin": {
         "b": 50,
         "l": 80,
         "r": 20,
         "t": 100
        },
        "plot_bgcolor": "white",
        "showlegend": true,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "rgb(36,36,36)"
            },
            "error_y": {
             "color": "rgb(36,36,36)"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "rgb(36,36,36)",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "rgb(36,36,36)"
            },
            "baxis": {
             "endlinecolor": "rgb(36,36,36)",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "rgb(36,36,36)"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.6
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 1,
              "tickcolor": "rgb(36,36,36)",
              "ticks": "outside"
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 1,
             "tickcolor": "rgb(36,36,36)",
             "ticks": "outside"
            },
            "colorscale": [
             [
              0,
              "#440154"
             ],
             [
              0.1111111111111111,
              "#482878"
             ],
             [
              0.2222222222222222,
              "#3e4989"
             ],
             [
              0.3333333333333333,
              "#31688e"
             ],
             [
              0.4444444444444444,
              "#26828e"
             ],
             [
              0.5555555555555556,
              "#1f9e89"
             ],
             [
              0.6666666666666666,
              "#35b779"
             ],
             [
              0.7777777777777778,
              "#6ece58"
             ],
             [
              0.8888888888888888,
              "#b5de2b"
             ],
             [
              1,
              "#fde725"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "rgb(237,237,237)"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "rgb(217,217,217)"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 1,
            "tickcolor": "rgb(36,36,36)",
            "ticks": "outside"
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "rgb(103,0,31)"
            ],
            [
             0.1,
             "rgb(178,24,43)"
            ],
            [
             0.2,
             "rgb(214,96,77)"
            ],
            [
             0.3,
             "rgb(244,165,130)"
            ],
            [
             0.4,
             "rgb(253,219,199)"
            ],
            [
             0.5,
             "rgb(247,247,247)"
            ],
            [
             0.6,
             "rgb(209,229,240)"
            ],
            [
             0.7,
             "rgb(146,197,222)"
            ],
            [
             0.8,
             "rgb(67,147,195)"
            ],
            [
             0.9,
             "rgb(33,102,172)"
            ],
            [
             1,
             "rgb(5,48,97)"
            ]
           ],
           "sequential": [
            [
             0,
             "#440154"
            ],
            [
             0.1111111111111111,
             "#482878"
            ],
            [
             0.2222222222222222,
             "#3e4989"
            ],
            [
             0.3333333333333333,
             "#31688e"
            ],
            [
             0.4444444444444444,
             "#26828e"
            ],
            [
             0.5555555555555556,
             "#1f9e89"
            ],
            [
             0.6666666666666666,
             "#35b779"
            ],
            [
             0.7777777777777778,
             "#6ece58"
            ],
            [
             0.8888888888888888,
             "#b5de2b"
            ],
            [
             1,
             "#fde725"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#440154"
            ],
            [
             0.1111111111111111,
             "#482878"
            ],
            [
             0.2222222222222222,
             "#3e4989"
            ],
            [
             0.3333333333333333,
             "#31688e"
            ],
            [
             0.4444444444444444,
             "#26828e"
            ],
            [
             0.5555555555555556,
             "#1f9e89"
            ],
            [
             0.6666666666666666,
             "#35b779"
            ],
            [
             0.7777777777777778,
             "#6ece58"
            ],
            [
             0.8888888888888888,
             "#b5de2b"
            ],
            [
             1,
             "#fde725"
            ]
           ]
          },
          "colorway": [
           "#1F77B4",
           "#FF7F0E",
           "#2CA02C",
           "#D62728",
           "#9467BD",
           "#8C564B",
           "#E377C2",
           "#7F7F7F",
           "#BCBD22",
           "#17BECF"
          ],
          "font": {
           "color": "rgb(36,36,36)"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "rgb(232,232,232)",
            "gridwidth": 2,
            "linecolor": "rgb(36,36,36)",
            "showbackground": true,
            "showgrid": false,
            "showline": true,
            "ticks": "outside",
            "zeroline": false,
            "zerolinecolor": "rgb(36,36,36)"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "rgb(232,232,232)",
            "gridwidth": 2,
            "linecolor": "rgb(36,36,36)",
            "showbackground": true,
            "showgrid": false,
            "showline": true,
            "ticks": "outside",
            "zeroline": false,
            "zerolinecolor": "rgb(36,36,36)"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "rgb(232,232,232)",
            "gridwidth": 2,
            "linecolor": "rgb(36,36,36)",
            "showbackground": true,
            "showgrid": false,
            "showline": true,
            "ticks": "outside",
            "zeroline": false,
            "zerolinecolor": "rgb(36,36,36)"
           }
          },
          "shapedefaults": {
           "fillcolor": "black",
           "line": {
            "width": 0
           },
           "opacity": 0.3
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           },
           "baxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "rgb(232,232,232)",
            "linecolor": "rgb(36,36,36)",
            "showgrid": false,
            "showline": true,
            "ticks": "outside"
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "rgb(232,232,232)",
           "linecolor": "rgb(36,36,36)",
           "showgrid": false,
           "showline": true,
           "ticks": "outside",
           "title": {
            "standoff": 15
           },
           "zeroline": false,
           "zerolinecolor": "rgb(36,36,36)"
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "rgb(232,232,232)",
           "linecolor": "rgb(36,36,36)",
           "showgrid": false,
           "showline": true,
           "ticks": "outside",
           "title": {
            "standoff": 15
           },
           "zeroline": false,
           "zerolinecolor": "rgb(36,36,36)"
          }
         }
        },
        "width": 720,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          0.175
         ],
         "showticklabels": false
        },
        "xaxis2": {
         "anchor": "y2",
         "domain": [
          0.275,
          0.45
         ],
         "showticklabels": false
        },
        "xaxis3": {
         "anchor": "y3",
         "domain": [
          0.55,
          0.7250000000000001
         ],
         "showticklabels": false
        },
        "xaxis4": {
         "anchor": "y4",
         "domain": [
          0.825,
          1
         ],
         "showticklabels": false
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "gridcolor": "lightgray",
         "range": [
          0,
          100
         ],
         "showgrid": true
        },
        "yaxis2": {
         "anchor": "x2",
         "domain": [
          0,
          1
         ],
         "gridcolor": "lightgray",
         "matches": "y",
         "range": [
          0,
          100
         ],
         "showgrid": true,
         "showticklabels": false
        },
        "yaxis3": {
         "anchor": "x3",
         "domain": [
          0,
          1
         ],
         "gridcolor": "lightgray",
         "matches": "y",
         "range": [
          0,
          100
         ],
         "showgrid": true,
         "showticklabels": false
        },
        "yaxis4": {
         "anchor": "x4",
         "domain": [
          0,
          1
         ],
         "gridcolor": "lightgray",
         "matches": "y",
         "range": [
          0,
          100
         ],
         "showgrid": true,
         "showticklabels": false
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plot_win_rates(summary_df_history)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9e251b80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['model', 'game_id', 'env', 'attempt', 'raw_guess', 'guess', 'response',\n",
       "       'word_limit', 'score', 'done', 'data', 'belief', 'action_reasoning',\n",
       "       'belief_reasoning'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "faa2c74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "wordle_sample = df[\n",
    "    (df['model'] == 'deepseek/deepseek-r1 (belief)') & \\\n",
    "        (df['env'] == 'twenty_questions') & \\\n",
    "            (df['game_id'] == str(0)) & \\\n",
    "            (df['info'] == 'belief')\n",
    "            ]\n",
    "print(len(wordle_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f9676af4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>game_id</th>\n",
       "      <th>env</th>\n",
       "      <th>attempt</th>\n",
       "      <th>info</th>\n",
       "      <th>raw_guess</th>\n",
       "      <th>guess</th>\n",
       "      <th>response</th>\n",
       "      <th>word_limit</th>\n",
       "      <th>score</th>\n",
       "      <th>done</th>\n",
       "      <th>data</th>\n",
       "      <th>belief</th>\n",
       "      <th>action_reasoning</th>\n",
       "      <th>belief_reasoning</th>\n",
       "      <th>model_info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>deepseek/deepseek-r1 (belief)</td>\n",
       "      <td>0</td>\n",
       "      <td>twenty_questions</td>\n",
       "      <td>1</td>\n",
       "      <td>belief</td>\n",
       "      <td>&lt;Think&gt; Start by distinguishing between common...</td>\n",
       "      <td>Is the vegetable typically grown underground?</td>\n",
       "      <td>yes.</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>{'agent_game_scenario': 'vegetable', 'env_game...</td>\n",
       "      <td>The vegetable is a root vegetable typically gr...</td>\n",
       "      <td>Okay, so I'm trying to guess a vegetable in 20...</td>\n",
       "      <td>Okay, so I need to update the belief state aft...</td>\n",
       "      <td>deepseek/deepseek-r1 (belief)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>deepseek/deepseek-r1 (belief)</td>\n",
       "      <td>0</td>\n",
       "      <td>twenty_questions</td>\n",
       "      <td>2</td>\n",
       "      <td>belief</td>\n",
       "      <td>&lt;Think&gt; Asking whether the vegetable is a bulb...</td>\n",
       "      <td>Is the vegetable a bulb, such as onion or garlic?</td>\n",
       "      <td>no.</td>\n",
       "      <td>None</td>\n",
       "      <td>2.0</td>\n",
       "      <td>True</td>\n",
       "      <td>{'agent_game_scenario': 'vegetable', 'env_game...</td>\n",
       "      <td>The vegetable is a root vegetable typically gr...</td>\n",
       "      <td>Okay, so the user is playing 20 Questions and ...</td>\n",
       "      <td>Okay, let's see. The user is playing 20 Questi...</td>\n",
       "      <td>deepseek/deepseek-r1 (belief)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>deepseek/deepseek-r1 (belief)</td>\n",
       "      <td>0</td>\n",
       "      <td>twenty_questions</td>\n",
       "      <td>3</td>\n",
       "      <td>belief</td>\n",
       "      <td>&lt;Think&gt; Asking whether the vegetable is a tube...</td>\n",
       "      <td>Is the vegetable classified as a tuber (e.g., ...</td>\n",
       "      <td>yes.</td>\n",
       "      <td>None</td>\n",
       "      <td>3.0</td>\n",
       "      <td>True</td>\n",
       "      <td>{'agent_game_scenario': 'vegetable', 'env_game...</td>\n",
       "      <td>The vegetable is confirmed to be a tuber (e.g....</td>\n",
       "      <td>Okay, let's see. The belief state says it's a ...</td>\n",
       "      <td>Okay, let's break this down. The user provided...</td>\n",
       "      <td>deepseek/deepseek-r1 (belief)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>deepseek/deepseek-r1 (belief)</td>\n",
       "      <td>0</td>\n",
       "      <td>twenty_questions</td>\n",
       "      <td>4</td>\n",
       "      <td>belief</td>\n",
       "      <td>&lt;Think&gt;Since the belief state confirms the veg...</td>\n",
       "      <td>Is the vegetable a potato?</td>\n",
       "      <td>no.</td>\n",
       "      <td>None</td>\n",
       "      <td>4.0</td>\n",
       "      <td>True</td>\n",
       "      <td>{'agent_game_scenario': 'vegetable', 'env_game...</td>\n",
       "      <td>The vegetable is confirmed to be a tuber, but ...</td>\n",
       "      <td>Okay, so the belief state says the vegetable i...</td>\n",
       "      <td>Okay, let's break down how to update the belie...</td>\n",
       "      <td>deepseek/deepseek-r1 (belief)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>deepseek/deepseek-r1 (belief)</td>\n",
       "      <td>0</td>\n",
       "      <td>twenty_questions</td>\n",
       "      <td>5</td>\n",
       "      <td>belief</td>\n",
       "      <td>&lt;Think&gt;\\nTo differentiate between the remainin...</td>\n",
       "      <td>Is the vegetable typically smaller than a pota...</td>\n",
       "      <td>yes.</td>\n",
       "      <td>None</td>\n",
       "      <td>5.0</td>\n",
       "      <td>True</td>\n",
       "      <td>{'agent_game_scenario': 'vegetable', 'env_game...</td>\n",
       "      <td>The vegetable is a tuber, confirmed not to be ...</td>\n",
       "      <td>Okay, let's see. The user is trying to figure ...</td>\n",
       "      <td>Okay, let me process this step by step. The cu...</td>\n",
       "      <td>deepseek/deepseek-r1 (belief)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>deepseek/deepseek-r1 (belief)</td>\n",
       "      <td>0</td>\n",
       "      <td>twenty_questions</td>\n",
       "      <td>6</td>\n",
       "      <td>belief</td>\n",
       "      <td>&lt;Think&gt; The belief strongly points to Jerusale...</td>\n",
       "      <td>Does the vegetable have thin, pale brown skin?</td>\n",
       "      <td>yes.</td>\n",
       "      <td>None</td>\n",
       "      <td>6.0</td>\n",
       "      <td>True</td>\n",
       "      <td>{'agent_game_scenario': 'vegetable', 'env_game...</td>\n",
       "      <td>The vegetable is a tuber smaller than a potato...</td>\n",
       "      <td>Okay, let's see. The belief state says the veg...</td>\n",
       "      <td>Okay, let me see. The user is playing 20 Quest...</td>\n",
       "      <td>deepseek/deepseek-r1 (belief)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>deepseek/deepseek-r1 (belief)</td>\n",
       "      <td>0</td>\n",
       "      <td>twenty_questions</td>\n",
       "      <td>7</td>\n",
       "      <td>belief</td>\n",
       "      <td>&lt;Think&gt; The vegetable is likely a sunchoke bas...</td>\n",
       "      <td>Is the vegetable known for having a slightly n...</td>\n",
       "      <td>yes.</td>\n",
       "      <td>None</td>\n",
       "      <td>7.0</td>\n",
       "      <td>True</td>\n",
       "      <td>{'agent_game_scenario': 'vegetable', 'env_game...</td>\n",
       "      <td>The vegetable is confirmed to have a slightly ...</td>\n",
       "      <td>Okay, so based on the belief state, the curren...</td>\n",
       "      <td>Okay, the user is playing 20 Questions trying ...</td>\n",
       "      <td>deepseek/deepseek-r1 (belief)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>deepseek/deepseek-r1 (belief)</td>\n",
       "      <td>0</td>\n",
       "      <td>twenty_questions</td>\n",
       "      <td>8</td>\n",
       "      <td>belief</td>\n",
       "      <td>&lt;Think&gt;The belief state strongly indicates Jer...</td>\n",
       "      <td>Is the vegetable a Jerusalem artichoke (suncho...</td>\n",
       "      <td>no.</td>\n",
       "      <td>None</td>\n",
       "      <td>8.0</td>\n",
       "      <td>True</td>\n",
       "      <td>{'agent_game_scenario': 'vegetable', 'env_game...</td>\n",
       "      <td>The vegetable is a small, knobby tuber with th...</td>\n",
       "      <td>Okay, so based on the belief state, the assist...</td>\n",
       "      <td>Okay, so the user was pretty confident that th...</td>\n",
       "      <td>deepseek/deepseek-r1 (belief)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>deepseek/deepseek-r1 (belief)</td>\n",
       "      <td>0</td>\n",
       "      <td>twenty_questions</td>\n",
       "      <td>9</td>\n",
       "      <td>belief</td>\n",
       "      <td>&lt;Think&gt;To distinguish between Yacon (eaten raw...</td>\n",
       "      <td>Is the vegetable commonly eaten raw?</td>\n",
       "      <td>no.</td>\n",
       "      <td>None</td>\n",
       "      <td>9.0</td>\n",
       "      <td>True</td>\n",
       "      <td>{'agent_game_scenario': 'vegetable', 'env_game...</td>\n",
       "      <td>The vegetable is a small, knobby tuber with th...</td>\n",
       "      <td>Okay, so I'm trying to figure out which vegeta...</td>\n",
       "      <td>Okay, let me try to update the belief state ba...</td>\n",
       "      <td>deepseek/deepseek-r1 (belief)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>deepseek/deepseek-r1 (belief)</td>\n",
       "      <td>0</td>\n",
       "      <td>twenty_questions</td>\n",
       "      <td>10</td>\n",
       "      <td>belief</td>\n",
       "      <td>&lt;Think&gt; To distinguish between Crosne and Chin...</td>\n",
       "      <td>Is the vegetable typically known for having a ...</td>\n",
       "      <td>yes.</td>\n",
       "      <td>None</td>\n",
       "      <td>10.0</td>\n",
       "      <td>True</td>\n",
       "      <td>{'agent_game_scenario': 'vegetable', 'env_game...</td>\n",
       "      <td>The vegetable is identified as **Crosne (Chine...</td>\n",
       "      <td>Okay, let's see. The belief state says the veg...</td>\n",
       "      <td>Okay, let me process this. The user is playing...</td>\n",
       "      <td>deepseek/deepseek-r1 (belief)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>deepseek/deepseek-r1 (belief)</td>\n",
       "      <td>0</td>\n",
       "      <td>twenty_questions</td>\n",
       "      <td>11</td>\n",
       "      <td>belief</td>\n",
       "      <td>&lt;Think&gt; Given the belief that the vegetable is...</td>\n",
       "      <td>Is the vegetable characterized by small, spira...</td>\n",
       "      <td>no.</td>\n",
       "      <td>None</td>\n",
       "      <td>11.0</td>\n",
       "      <td>True</td>\n",
       "      <td>{'agent_game_scenario': 'vegetable', 'env_game...</td>\n",
       "      <td>The vegetable is **not Crosne** (spiral-shaped...</td>\n",
       "      <td>Okay, let me try to figure out the next step h...</td>\n",
       "      <td>Okay, let's process this. The current belief w...</td>\n",
       "      <td>deepseek/deepseek-r1 (belief)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>deepseek/deepseek-r1 (belief)</td>\n",
       "      <td>0</td>\n",
       "      <td>twenty_questions</td>\n",
       "      <td>12</td>\n",
       "      <td>belief</td>\n",
       "      <td>&lt;Think&gt; Since the key candidates (water chestn...</td>\n",
       "      <td>Is it round and smooth?</td>\n",
       "      <td>no.</td>\n",
       "      <td>None</td>\n",
       "      <td>12.0</td>\n",
       "      <td>True</td>\n",
       "      <td>{'agent_game_scenario': 'vegetable', 'env_game...</td>\n",
       "      <td>The vegetable is **not Crosne**, potato, carro...</td>\n",
       "      <td>Okay, let's see. The user is trying to guess a...</td>\n",
       "      <td>Okay, so the user is playing 20 Questions, try...</td>\n",
       "      <td>deepseek/deepseek-r1 (belief)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>deepseek/deepseek-r1 (belief)</td>\n",
       "      <td>0</td>\n",
       "      <td>twenty_questions</td>\n",
       "      <td>13</td>\n",
       "      <td>belief</td>\n",
       "      <td>&lt;Think&gt; Both candidates are knobby, so confirm...</td>\n",
       "      <td>Is the vegetable commonly used in Asian cuisine?</td>\n",
       "      <td>yes.</td>\n",
       "      <td>None</td>\n",
       "      <td>13.0</td>\n",
       "      <td>True</td>\n",
       "      <td>{'agent_game_scenario': 'vegetable', 'env_game...</td>\n",
       "      <td>Current belief: The vegetable is **not** Crosn...</td>\n",
       "      <td>Okay, let's see. The user is trying to guess a...</td>\n",
       "      <td>Okay, let me go through this step by step. The...</td>\n",
       "      <td>deepseek/deepseek-r1 (belief)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>deepseek/deepseek-r1 (belief)</td>\n",
       "      <td>0</td>\n",
       "      <td>twenty_questions</td>\n",
       "      <td>14</td>\n",
       "      <td>belief</td>\n",
       "      <td>&lt;Think&gt; The belief state indicates burdock roo...</td>\n",
       "      <td>Guess: Burdock root (gobo)</td>\n",
       "      <td>no.</td>\n",
       "      <td>None</td>\n",
       "      <td>14.0</td>\n",
       "      <td>True</td>\n",
       "      <td>{'agent_game_scenario': 'vegetable', 'env_game...</td>\n",
       "      <td>&lt;BELIEW&gt;\\nCurrent belief: The vegetable is **n...</td>\n",
       "      <td>Okay, let's break this down. The belief state ...</td>\n",
       "      <td>Okay, let's see. The user was trying to guess ...</td>\n",
       "      <td>deepseek/deepseek-r1 (belief)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>deepseek/deepseek-r1 (belief)</td>\n",
       "      <td>0</td>\n",
       "      <td>twenty_questions</td>\n",
       "      <td>15</td>\n",
       "      <td>belief</td>\n",
       "      <td>&lt;Think&gt; The belief state strongly points to lo...</td>\n",
       "      <td>Is the vegetable lotus root?</td>\n",
       "      <td>no.</td>\n",
       "      <td>None</td>\n",
       "      <td>15.0</td>\n",
       "      <td>True</td>\n",
       "      <td>{'agent_game_scenario': 'vegetable', 'env_game...</td>\n",
       "      <td>The vegetable is **not** Burdock root (gobo), ...</td>\n",
       "      <td>Okay, so the belief state says they've narrowe...</td>\n",
       "      <td>Okay, so the user is playing 20 Questions tryi...</td>\n",
       "      <td>deepseek/deepseek-r1 (belief)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569</th>\n",
       "      <td>deepseek/deepseek-r1 (belief)</td>\n",
       "      <td>0</td>\n",
       "      <td>twenty_questions</td>\n",
       "      <td>16</td>\n",
       "      <td>belief</td>\n",
       "      <td>&lt;Think&gt; To differentiate between water caltrop...</td>\n",
       "      <td>Is the vegetable characterized by horn-shaped ...</td>\n",
       "      <td>no.</td>\n",
       "      <td>None</td>\n",
       "      <td>16.0</td>\n",
       "      <td>True</td>\n",
       "      <td>{'agent_game_scenario': 'vegetable', 'env_game...</td>\n",
       "      <td>The vegetable is **not** Burdock root, Crosne,...</td>\n",
       "      <td>Okay, let's see. The user is trying to guess a...</td>\n",
       "      <td>Okay, let's see. The current belief says the v...</td>\n",
       "      <td>deepseek/deepseek-r1 (belief)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>deepseek/deepseek-r1 (belief)</td>\n",
       "      <td>0</td>\n",
       "      <td>twenty_questions</td>\n",
       "      <td>17</td>\n",
       "      <td>belief</td>\n",
       "      <td>&lt;Think&gt; The key distinction is whether the veg...</td>\n",
       "      <td>Is the vegetable primarily used as a spice or ...</td>\n",
       "      <td>no.</td>\n",
       "      <td>None</td>\n",
       "      <td>17.0</td>\n",
       "      <td>True</td>\n",
       "      <td>{'agent_game_scenario': 'vegetable', 'env_game...</td>\n",
       "      <td>The vegetable is **not** Burdock root, Crosne,...</td>\n",
       "      <td>Okay, let's see. The user has narrowed down th...</td>\n",
       "      <td>Okay, let me update the belief based on the la...</td>\n",
       "      <td>deepseek/deepseek-r1 (belief)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571</th>\n",
       "      <td>deepseek/deepseek-r1 (belief)</td>\n",
       "      <td>0</td>\n",
       "      <td>twenty_questions</td>\n",
       "      <td>18</td>\n",
       "      <td>belief</td>\n",
       "      <td>&lt;Think&gt;Since crunchiness when cooked is confir...</td>\n",
       "      <td>Is the vegetable commonly stir-fried in Asian ...</td>\n",
       "      <td>yes.</td>\n",
       "      <td>None</td>\n",
       "      <td>18.0</td>\n",
       "      <td>True</td>\n",
       "      <td>{'agent_game_scenario': 'vegetable', 'env_game...</td>\n",
       "      <td>The vegetable is **not** Burdock root, Crosne,...</td>\n",
       "      <td>Okay, let's break this down. The user is tryin...</td>\n",
       "      <td>Okay, let's see. The user is playing 20 Questi...</td>\n",
       "      <td>deepseek/deepseek-r1 (belief)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>deepseek/deepseek-r1 (belief)</td>\n",
       "      <td>0</td>\n",
       "      <td>twenty_questions</td>\n",
       "      <td>19</td>\n",
       "      <td>belief</td>\n",
       "      <td>&lt;Think&gt; Considering the narrowed possibilities...</td>\n",
       "      <td>Is the vegetable a gourd (e.g., ivy gourd)?</td>\n",
       "      <td>no.</td>\n",
       "      <td>None</td>\n",
       "      <td>19.0</td>\n",
       "      <td>True</td>\n",
       "      <td>{'agent_game_scenario': 'vegetable', 'env_game...</td>\n",
       "      <td>The vegetable is **not** Burdock root, Crosne,...</td>\n",
       "      <td>Okay, let me process the information given. Th...</td>\n",
       "      <td>Okay, let's break this down. The user is playi...</td>\n",
       "      <td>deepseek/deepseek-r1 (belief)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573</th>\n",
       "      <td>deepseek/deepseek-r1 (belief)</td>\n",
       "      <td>0</td>\n",
       "      <td>twenty_questions</td>\n",
       "      <td>20</td>\n",
       "      <td>belief</td>\n",
       "      <td>&lt;Think&gt;  \\nTo narrow down between the remainin...</td>\n",
       "      <td>Is the vegetable commonly recognized by a wing...</td>\n",
       "      <td>no.</td>\n",
       "      <td>None</td>\n",
       "      <td>20.0</td>\n",
       "      <td>True</td>\n",
       "      <td>{'agent_game_scenario': 'vegetable', 'env_game...</td>\n",
       "      <td>The vegetable is **not** Burdock root, Crosne,...</td>\n",
       "      <td>Okay, let's break this down. The user is tryin...</td>\n",
       "      <td>Okay, so the user is playing 20 Questions tryi...</td>\n",
       "      <td>deepseek/deepseek-r1 (belief)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             model game_id               env  attempt    info  \\\n",
       "554  deepseek/deepseek-r1 (belief)       0  twenty_questions        1  belief   \n",
       "555  deepseek/deepseek-r1 (belief)       0  twenty_questions        2  belief   \n",
       "556  deepseek/deepseek-r1 (belief)       0  twenty_questions        3  belief   \n",
       "557  deepseek/deepseek-r1 (belief)       0  twenty_questions        4  belief   \n",
       "558  deepseek/deepseek-r1 (belief)       0  twenty_questions        5  belief   \n",
       "559  deepseek/deepseek-r1 (belief)       0  twenty_questions        6  belief   \n",
       "560  deepseek/deepseek-r1 (belief)       0  twenty_questions        7  belief   \n",
       "561  deepseek/deepseek-r1 (belief)       0  twenty_questions        8  belief   \n",
       "562  deepseek/deepseek-r1 (belief)       0  twenty_questions        9  belief   \n",
       "563  deepseek/deepseek-r1 (belief)       0  twenty_questions       10  belief   \n",
       "564  deepseek/deepseek-r1 (belief)       0  twenty_questions       11  belief   \n",
       "565  deepseek/deepseek-r1 (belief)       0  twenty_questions       12  belief   \n",
       "566  deepseek/deepseek-r1 (belief)       0  twenty_questions       13  belief   \n",
       "567  deepseek/deepseek-r1 (belief)       0  twenty_questions       14  belief   \n",
       "568  deepseek/deepseek-r1 (belief)       0  twenty_questions       15  belief   \n",
       "569  deepseek/deepseek-r1 (belief)       0  twenty_questions       16  belief   \n",
       "570  deepseek/deepseek-r1 (belief)       0  twenty_questions       17  belief   \n",
       "571  deepseek/deepseek-r1 (belief)       0  twenty_questions       18  belief   \n",
       "572  deepseek/deepseek-r1 (belief)       0  twenty_questions       19  belief   \n",
       "573  deepseek/deepseek-r1 (belief)       0  twenty_questions       20  belief   \n",
       "\n",
       "                                             raw_guess  \\\n",
       "554  <Think> Start by distinguishing between common...   \n",
       "555  <Think> Asking whether the vegetable is a bulb...   \n",
       "556  <Think> Asking whether the vegetable is a tube...   \n",
       "557  <Think>Since the belief state confirms the veg...   \n",
       "558  <Think>\\nTo differentiate between the remainin...   \n",
       "559  <Think> The belief strongly points to Jerusale...   \n",
       "560  <Think> The vegetable is likely a sunchoke bas...   \n",
       "561  <Think>The belief state strongly indicates Jer...   \n",
       "562  <Think>To distinguish between Yacon (eaten raw...   \n",
       "563  <Think> To distinguish between Crosne and Chin...   \n",
       "564  <Think> Given the belief that the vegetable is...   \n",
       "565  <Think> Since the key candidates (water chestn...   \n",
       "566  <Think> Both candidates are knobby, so confirm...   \n",
       "567  <Think> The belief state indicates burdock roo...   \n",
       "568  <Think> The belief state strongly points to lo...   \n",
       "569  <Think> To differentiate between water caltrop...   \n",
       "570  <Think> The key distinction is whether the veg...   \n",
       "571  <Think>Since crunchiness when cooked is confir...   \n",
       "572  <Think> Considering the narrowed possibilities...   \n",
       "573  <Think>  \\nTo narrow down between the remainin...   \n",
       "\n",
       "                                                 guess response word_limit  \\\n",
       "554      Is the vegetable typically grown underground?     yes.       None   \n",
       "555  Is the vegetable a bulb, such as onion or garlic?      no.       None   \n",
       "556  Is the vegetable classified as a tuber (e.g., ...     yes.       None   \n",
       "557                         Is the vegetable a potato?      no.       None   \n",
       "558  Is the vegetable typically smaller than a pota...     yes.       None   \n",
       "559     Does the vegetable have thin, pale brown skin?     yes.       None   \n",
       "560  Is the vegetable known for having a slightly n...     yes.       None   \n",
       "561  Is the vegetable a Jerusalem artichoke (suncho...      no.       None   \n",
       "562               Is the vegetable commonly eaten raw?      no.       None   \n",
       "563  Is the vegetable typically known for having a ...     yes.       None   \n",
       "564  Is the vegetable characterized by small, spira...      no.       None   \n",
       "565                            Is it round and smooth?      no.       None   \n",
       "566   Is the vegetable commonly used in Asian cuisine?     yes.       None   \n",
       "567                         Guess: Burdock root (gobo)      no.       None   \n",
       "568                       Is the vegetable lotus root?      no.       None   \n",
       "569  Is the vegetable characterized by horn-shaped ...      no.       None   \n",
       "570  Is the vegetable primarily used as a spice or ...      no.       None   \n",
       "571  Is the vegetable commonly stir-fried in Asian ...     yes.       None   \n",
       "572        Is the vegetable a gourd (e.g., ivy gourd)?      no.       None   \n",
       "573  Is the vegetable commonly recognized by a wing...      no.       None   \n",
       "\n",
       "     score  done                                               data  \\\n",
       "554    1.0  True  {'agent_game_scenario': 'vegetable', 'env_game...   \n",
       "555    2.0  True  {'agent_game_scenario': 'vegetable', 'env_game...   \n",
       "556    3.0  True  {'agent_game_scenario': 'vegetable', 'env_game...   \n",
       "557    4.0  True  {'agent_game_scenario': 'vegetable', 'env_game...   \n",
       "558    5.0  True  {'agent_game_scenario': 'vegetable', 'env_game...   \n",
       "559    6.0  True  {'agent_game_scenario': 'vegetable', 'env_game...   \n",
       "560    7.0  True  {'agent_game_scenario': 'vegetable', 'env_game...   \n",
       "561    8.0  True  {'agent_game_scenario': 'vegetable', 'env_game...   \n",
       "562    9.0  True  {'agent_game_scenario': 'vegetable', 'env_game...   \n",
       "563   10.0  True  {'agent_game_scenario': 'vegetable', 'env_game...   \n",
       "564   11.0  True  {'agent_game_scenario': 'vegetable', 'env_game...   \n",
       "565   12.0  True  {'agent_game_scenario': 'vegetable', 'env_game...   \n",
       "566   13.0  True  {'agent_game_scenario': 'vegetable', 'env_game...   \n",
       "567   14.0  True  {'agent_game_scenario': 'vegetable', 'env_game...   \n",
       "568   15.0  True  {'agent_game_scenario': 'vegetable', 'env_game...   \n",
       "569   16.0  True  {'agent_game_scenario': 'vegetable', 'env_game...   \n",
       "570   17.0  True  {'agent_game_scenario': 'vegetable', 'env_game...   \n",
       "571   18.0  True  {'agent_game_scenario': 'vegetable', 'env_game...   \n",
       "572   19.0  True  {'agent_game_scenario': 'vegetable', 'env_game...   \n",
       "573   20.0  True  {'agent_game_scenario': 'vegetable', 'env_game...   \n",
       "\n",
       "                                                belief  \\\n",
       "554  The vegetable is a root vegetable typically gr...   \n",
       "555  The vegetable is a root vegetable typically gr...   \n",
       "556  The vegetable is confirmed to be a tuber (e.g....   \n",
       "557  The vegetable is confirmed to be a tuber, but ...   \n",
       "558  The vegetable is a tuber, confirmed not to be ...   \n",
       "559  The vegetable is a tuber smaller than a potato...   \n",
       "560  The vegetable is confirmed to have a slightly ...   \n",
       "561  The vegetable is a small, knobby tuber with th...   \n",
       "562  The vegetable is a small, knobby tuber with th...   \n",
       "563  The vegetable is identified as **Crosne (Chine...   \n",
       "564  The vegetable is **not Crosne** (spiral-shaped...   \n",
       "565  The vegetable is **not Crosne**, potato, carro...   \n",
       "566  Current belief: The vegetable is **not** Crosn...   \n",
       "567  <BELIEW>\\nCurrent belief: The vegetable is **n...   \n",
       "568  The vegetable is **not** Burdock root (gobo), ...   \n",
       "569  The vegetable is **not** Burdock root, Crosne,...   \n",
       "570  The vegetable is **not** Burdock root, Crosne,...   \n",
       "571  The vegetable is **not** Burdock root, Crosne,...   \n",
       "572  The vegetable is **not** Burdock root, Crosne,...   \n",
       "573  The vegetable is **not** Burdock root, Crosne,...   \n",
       "\n",
       "                                      action_reasoning  \\\n",
       "554  Okay, so I'm trying to guess a vegetable in 20...   \n",
       "555  Okay, so the user is playing 20 Questions and ...   \n",
       "556  Okay, let's see. The belief state says it's a ...   \n",
       "557  Okay, so the belief state says the vegetable i...   \n",
       "558  Okay, let's see. The user is trying to figure ...   \n",
       "559  Okay, let's see. The belief state says the veg...   \n",
       "560  Okay, so based on the belief state, the curren...   \n",
       "561  Okay, so based on the belief state, the assist...   \n",
       "562  Okay, so I'm trying to figure out which vegeta...   \n",
       "563  Okay, let's see. The belief state says the veg...   \n",
       "564  Okay, let me try to figure out the next step h...   \n",
       "565  Okay, let's see. The user is trying to guess a...   \n",
       "566  Okay, let's see. The user is trying to guess a...   \n",
       "567  Okay, let's break this down. The belief state ...   \n",
       "568  Okay, so the belief state says they've narrowe...   \n",
       "569  Okay, let's see. The user is trying to guess a...   \n",
       "570  Okay, let's see. The user has narrowed down th...   \n",
       "571  Okay, let's break this down. The user is tryin...   \n",
       "572  Okay, let me process the information given. Th...   \n",
       "573  Okay, let's break this down. The user is tryin...   \n",
       "\n",
       "                                      belief_reasoning  \\\n",
       "554  Okay, so I need to update the belief state aft...   \n",
       "555  Okay, let's see. The user is playing 20 Questi...   \n",
       "556  Okay, let's break this down. The user provided...   \n",
       "557  Okay, let's break down how to update the belie...   \n",
       "558  Okay, let me process this step by step. The cu...   \n",
       "559  Okay, let me see. The user is playing 20 Quest...   \n",
       "560  Okay, the user is playing 20 Questions trying ...   \n",
       "561  Okay, so the user was pretty confident that th...   \n",
       "562  Okay, let me try to update the belief state ba...   \n",
       "563  Okay, let me process this. The user is playing...   \n",
       "564  Okay, let's process this. The current belief w...   \n",
       "565  Okay, so the user is playing 20 Questions, try...   \n",
       "566  Okay, let me go through this step by step. The...   \n",
       "567  Okay, let's see. The user was trying to guess ...   \n",
       "568  Okay, so the user is playing 20 Questions tryi...   \n",
       "569  Okay, let's see. The current belief says the v...   \n",
       "570  Okay, let me update the belief based on the la...   \n",
       "571  Okay, let's see. The user is playing 20 Questi...   \n",
       "572  Okay, let's break this down. The user is playi...   \n",
       "573  Okay, so the user is playing 20 Questions tryi...   \n",
       "\n",
       "                        model_info  \n",
       "554  deepseek/deepseek-r1 (belief)  \n",
       "555  deepseek/deepseek-r1 (belief)  \n",
       "556  deepseek/deepseek-r1 (belief)  \n",
       "557  deepseek/deepseek-r1 (belief)  \n",
       "558  deepseek/deepseek-r1 (belief)  \n",
       "559  deepseek/deepseek-r1 (belief)  \n",
       "560  deepseek/deepseek-r1 (belief)  \n",
       "561  deepseek/deepseek-r1 (belief)  \n",
       "562  deepseek/deepseek-r1 (belief)  \n",
       "563  deepseek/deepseek-r1 (belief)  \n",
       "564  deepseek/deepseek-r1 (belief)  \n",
       "565  deepseek/deepseek-r1 (belief)  \n",
       "566  deepseek/deepseek-r1 (belief)  \n",
       "567  deepseek/deepseek-r1 (belief)  \n",
       "568  deepseek/deepseek-r1 (belief)  \n",
       "569  deepseek/deepseek-r1 (belief)  \n",
       "570  deepseek/deepseek-r1 (belief)  \n",
       "571  deepseek/deepseek-r1 (belief)  \n",
       "572  deepseek/deepseek-r1 (belief)  \n",
       "573  deepseek/deepseek-r1 (belief)  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordle_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2af8aebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The vegetable is **not** Burdock root, Crosne, potato, carrot, beet, Chinese '\n",
      " 'yam, water chestnut, jicama, Jerusalem artichoke, lotus root, water caltrop, '\n",
      " 'galangal, **gourd (including ivy gourd)**, Hosui pear-shaped yam, **or '\n",
      " 'winged yam (Dioscorea alata)**.  \\n'\n",
      " '**Confirmed traits**:  \\n'\n",
      " '- Crunchy when cooked.  \\n'\n",
      " '- Small, knobby, non-round/non-smooth appearance.  \\n'\n",
      " '- Pale brown skin with nutty/sweet flavor.  \\n'\n",
      " '- Asian origin (common in Asian cuisine).  \\n'\n",
      " '- Primarily used as a culinary vegetable (**not** a spice/herb).  \\n'\n",
      " '- Commonly stir-fried in Asian dishes.  \\n'\n",
      " '**Narrowed possibilities**:  \\n'\n",
      " '- Excludes winged/lobed shapes. Focus on knobby, irregularly shaped '\n",
      " 'tubers/roots.  \\n'\n",
      " '- Candidates: Lesser-known Southeast Asian tubers (e.g., *greater yam '\n",
      " 'variants*, *Taro cultivars* with knobby texture) or fibrous-textured roots '\n",
      " 'used in Thai/Malay/Indonesian stir-fries.  \\n'\n",
      " '- Key questions remaining: Distinguish between starchy vs. fibrous texture, '\n",
      " 'regional specificity (e.g., Thai vs. Filipino cuisine), or cooking methods '\n",
      " 'beyond stir-fry (e.g., soups, pickling).')\n"
     ]
    }
   ],
   "source": [
    "pp(wordle_sample.iloc[19]['belief'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb344f62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8799a5ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495517be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c57f8f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df4c224",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90539d27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "verl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
